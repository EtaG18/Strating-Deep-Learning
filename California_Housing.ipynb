{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the California housing dataset\n",
        "california = fetch_california_housing()\n",
        "\n",
        "# Split the data into features and target\n",
        "X = california.data\n",
        "y = california.target\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBCek3pJVtAo",
        "outputId": "e8914aa3-29cf-4a67-d008-bdf63bcd5507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "Train_loader = DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=128)\n",
        "Test_loader = DataLoader(list(zip(X_test, y_test)), shuffle=False, batch_size=16)\n",
        "\n",
        "Training_loss=[]\n",
        "Validation_loss=[]\n",
        "Training_accuracy=[]\n",
        "Validation_accuracy=[]\n",
        "\n",
        "# Define the MultiLayer Perceptron model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(6, 4)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(4, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(2,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# Function to calculate precision, recall, and f1-score\n",
        "def precision_recall(model, criterion, optimizer, Test_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in Test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted',labels=np.unique(y_pred))\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "# Function to train the model\n",
        "def train(model, criterion, optimizer, Train_loader):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in Train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        labels =  labels.type(torch.LongTensor)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    epoch_loss = running_loss / len(Train_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    Training_loss.append(epoch_loss)\n",
        "    Training_accuracy.append(epoch_accuracy)\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "# Function to validate the model\n",
        "def validate(model, criterion, Test_loader):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in Test_loader:\n",
        "            labels =  labels.type(torch.LongTensor)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    epoch_loss = running_loss / len(Test_loader)\n",
        "    epoch_accuracy = 100 * correct / total\n",
        "    Validation_loss.append(epoch_loss)\n",
        "    Validation_accuracy.append(epoch_accuracy)\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = 8                                                                  # Enter the input size (number of features)                                                                 # Enter the number of hidden units                                                         # Number of classes in the dataset\n",
        "lr = 0.001                                                                      # Learning rate\n",
        "num_epochs = 80\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = MLP(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train(model, criterion, optimizer, Train_loader)\n",
        "    val_loss, val_accuracy = validate(model, criterion, Test_loader)\n",
        "    precision, recall, _ = precision_recall(model, criterion, optimizer, Test_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, \"\n",
        "          f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "# Calculate and print confusion matrix\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in Test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "OHDD8BPBXq_C",
        "outputId": "997a89db-9035-4c53-8afc-1be94ca955ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-900c35babd98>\u001b[0m in \u001b[0;36m<cell line: 113>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-900c35babd98>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, Train_loader)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "data = housing.data\n",
        "target = housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Define the regression model\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(6, 4)\n",
        "        self.fc3 = nn.Linear(4, 2)\n",
        "        self.fc4 = nn.Linear(2, 1)  # Output layer with one neuron (regression)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "input_size = X_train_scaled.shape[1]\n",
        "model = RegressionModel(input_size)\n",
        "\n",
        "# Loss function (Mean Squared Error) and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1500\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate MAE and RMSE for evaluation\n",
        "    with torch.no_grad():\n",
        "        predicted = model(X_test_tensor)\n",
        "        mae = mean_absolute_error(y_test, predicted.numpy())\n",
        "        rmse = mean_squared_error(y_test, predicted.numpy(), squared=False)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    test_loss = criterion(test_outputs, y_test_tensor)\n",
        "    mae = mean_absolute_error(y_test, test_outputs.numpy())\n",
        "    rmse = mean_squared_error(y_test, test_outputs.numpy(), squared=False)\n",
        "\n",
        "print(f'Test Loss: {test_loss.item():.4f}, Test MAE: {mae:.4f}, Test RMSE: {rmse:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWSduy_Zcs8V",
        "outputId": "277fb807-85b9-450c-c2a2-12b61fd11c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1500], Loss: 3.9462, MAE: 1.5964, RMSE: 1.9595\n",
            "Epoch [2/1500], Loss: 3.9256, MAE: 1.5903, RMSE: 1.9543\n",
            "Epoch [3/1500], Loss: 3.9051, MAE: 1.5843, RMSE: 1.9492\n",
            "Epoch [4/1500], Loss: 3.8847, MAE: 1.5783, RMSE: 1.9440\n",
            "Epoch [5/1500], Loss: 3.8643, MAE: 1.5723, RMSE: 1.9389\n",
            "Epoch [6/1500], Loss: 3.8441, MAE: 1.5663, RMSE: 1.9338\n",
            "Epoch [7/1500], Loss: 3.8239, MAE: 1.5603, RMSE: 1.9287\n",
            "Epoch [8/1500], Loss: 3.8039, MAE: 1.5543, RMSE: 1.9236\n",
            "Epoch [9/1500], Loss: 3.7839, MAE: 1.5484, RMSE: 1.9185\n",
            "Epoch [10/1500], Loss: 3.7641, MAE: 1.5424, RMSE: 1.9134\n",
            "Epoch [11/1500], Loss: 3.7443, MAE: 1.5364, RMSE: 1.9083\n",
            "Epoch [12/1500], Loss: 3.7246, MAE: 1.5305, RMSE: 1.9033\n",
            "Epoch [13/1500], Loss: 3.7051, MAE: 1.5246, RMSE: 1.8983\n",
            "Epoch [14/1500], Loss: 3.6856, MAE: 1.5187, RMSE: 1.8932\n",
            "Epoch [15/1500], Loss: 3.6662, MAE: 1.5128, RMSE: 1.8882\n",
            "Epoch [16/1500], Loss: 3.6469, MAE: 1.5069, RMSE: 1.8832\n",
            "Epoch [17/1500], Loss: 3.6278, MAE: 1.5011, RMSE: 1.8782\n",
            "Epoch [18/1500], Loss: 3.6087, MAE: 1.4952, RMSE: 1.8732\n",
            "Epoch [19/1500], Loss: 3.5897, MAE: 1.4894, RMSE: 1.8683\n",
            "Epoch [20/1500], Loss: 3.5708, MAE: 1.4836, RMSE: 1.8633\n",
            "Epoch [21/1500], Loss: 3.5520, MAE: 1.4778, RMSE: 1.8584\n",
            "Epoch [22/1500], Loss: 3.5333, MAE: 1.4721, RMSE: 1.8534\n",
            "Epoch [23/1500], Loss: 3.5147, MAE: 1.4663, RMSE: 1.8485\n",
            "Epoch [24/1500], Loss: 3.4962, MAE: 1.4606, RMSE: 1.8436\n",
            "Epoch [25/1500], Loss: 3.4777, MAE: 1.4549, RMSE: 1.8387\n",
            "Epoch [26/1500], Loss: 3.4594, MAE: 1.4492, RMSE: 1.8338\n",
            "Epoch [27/1500], Loss: 3.4411, MAE: 1.4435, RMSE: 1.8289\n",
            "Epoch [28/1500], Loss: 3.4230, MAE: 1.4378, RMSE: 1.8241\n",
            "Epoch [29/1500], Loss: 3.4049, MAE: 1.4322, RMSE: 1.8192\n",
            "Epoch [30/1500], Loss: 3.3869, MAE: 1.4266, RMSE: 1.8143\n",
            "Epoch [31/1500], Loss: 3.3690, MAE: 1.4210, RMSE: 1.8095\n",
            "Epoch [32/1500], Loss: 3.3511, MAE: 1.4154, RMSE: 1.8047\n",
            "Epoch [33/1500], Loss: 3.3334, MAE: 1.4098, RMSE: 1.7998\n",
            "Epoch [34/1500], Loss: 3.3157, MAE: 1.4043, RMSE: 1.7950\n",
            "Epoch [35/1500], Loss: 3.2981, MAE: 1.3988, RMSE: 1.7902\n",
            "Epoch [36/1500], Loss: 3.2806, MAE: 1.3933, RMSE: 1.7854\n",
            "Epoch [37/1500], Loss: 3.2631, MAE: 1.3878, RMSE: 1.7806\n",
            "Epoch [38/1500], Loss: 3.2457, MAE: 1.3823, RMSE: 1.7759\n",
            "Epoch [39/1500], Loss: 3.2284, MAE: 1.3769, RMSE: 1.7711\n",
            "Epoch [40/1500], Loss: 3.2112, MAE: 1.3715, RMSE: 1.7663\n",
            "Epoch [41/1500], Loss: 3.1940, MAE: 1.3660, RMSE: 1.7615\n",
            "Epoch [42/1500], Loss: 3.1769, MAE: 1.3606, RMSE: 1.7568\n",
            "Epoch [43/1500], Loss: 3.1599, MAE: 1.3552, RMSE: 1.7520\n",
            "Epoch [44/1500], Loss: 3.1429, MAE: 1.3498, RMSE: 1.7473\n",
            "Epoch [45/1500], Loss: 3.1260, MAE: 1.3445, RMSE: 1.7425\n",
            "Epoch [46/1500], Loss: 3.1091, MAE: 1.3391, RMSE: 1.7378\n",
            "Epoch [47/1500], Loss: 3.0923, MAE: 1.3337, RMSE: 1.7330\n",
            "Epoch [48/1500], Loss: 3.0756, MAE: 1.3284, RMSE: 1.7283\n",
            "Epoch [49/1500], Loss: 3.0589, MAE: 1.3230, RMSE: 1.7236\n",
            "Epoch [50/1500], Loss: 3.0422, MAE: 1.3177, RMSE: 1.7188\n",
            "Epoch [51/1500], Loss: 3.0257, MAE: 1.3124, RMSE: 1.7141\n",
            "Epoch [52/1500], Loss: 3.0091, MAE: 1.3071, RMSE: 1.7094\n",
            "Epoch [53/1500], Loss: 2.9927, MAE: 1.3018, RMSE: 1.7046\n",
            "Epoch [54/1500], Loss: 2.9762, MAE: 1.2965, RMSE: 1.6999\n",
            "Epoch [55/1500], Loss: 2.9599, MAE: 1.2912, RMSE: 1.6952\n",
            "Epoch [56/1500], Loss: 2.9435, MAE: 1.2860, RMSE: 1.6905\n",
            "Epoch [57/1500], Loss: 2.9273, MAE: 1.2808, RMSE: 1.6857\n",
            "Epoch [58/1500], Loss: 2.9110, MAE: 1.2756, RMSE: 1.6810\n",
            "Epoch [59/1500], Loss: 2.8948, MAE: 1.2704, RMSE: 1.6763\n",
            "Epoch [60/1500], Loss: 2.8787, MAE: 1.2651, RMSE: 1.6716\n",
            "Epoch [61/1500], Loss: 2.8626, MAE: 1.2599, RMSE: 1.6669\n",
            "Epoch [62/1500], Loss: 2.8465, MAE: 1.2547, RMSE: 1.6621\n",
            "Epoch [63/1500], Loss: 2.8305, MAE: 1.2496, RMSE: 1.6574\n",
            "Epoch [64/1500], Loss: 2.8145, MAE: 1.2444, RMSE: 1.6527\n",
            "Epoch [65/1500], Loss: 2.7986, MAE: 1.2393, RMSE: 1.6479\n",
            "Epoch [66/1500], Loss: 2.7826, MAE: 1.2341, RMSE: 1.6432\n",
            "Epoch [67/1500], Loss: 2.7668, MAE: 1.2290, RMSE: 1.6385\n",
            "Epoch [68/1500], Loss: 2.7509, MAE: 1.2239, RMSE: 1.6337\n",
            "Epoch [69/1500], Loss: 2.7351, MAE: 1.2188, RMSE: 1.6290\n",
            "Epoch [70/1500], Loss: 2.7194, MAE: 1.2137, RMSE: 1.6243\n",
            "Epoch [71/1500], Loss: 2.7036, MAE: 1.2087, RMSE: 1.6195\n",
            "Epoch [72/1500], Loss: 2.6879, MAE: 1.2036, RMSE: 1.6148\n",
            "Epoch [73/1500], Loss: 2.6723, MAE: 1.1985, RMSE: 1.6100\n",
            "Epoch [74/1500], Loss: 2.6566, MAE: 1.1935, RMSE: 1.6052\n",
            "Epoch [75/1500], Loss: 2.6410, MAE: 1.1884, RMSE: 1.6005\n",
            "Epoch [76/1500], Loss: 2.6255, MAE: 1.1834, RMSE: 1.5957\n",
            "Epoch [77/1500], Loss: 2.6099, MAE: 1.1784, RMSE: 1.5909\n",
            "Epoch [78/1500], Loss: 2.5944, MAE: 1.1734, RMSE: 1.5862\n",
            "Epoch [79/1500], Loss: 2.5789, MAE: 1.1684, RMSE: 1.5814\n",
            "Epoch [80/1500], Loss: 2.5635, MAE: 1.1634, RMSE: 1.5766\n",
            "Epoch [81/1500], Loss: 2.5480, MAE: 1.1584, RMSE: 1.5718\n",
            "Epoch [82/1500], Loss: 2.5326, MAE: 1.1535, RMSE: 1.5670\n",
            "Epoch [83/1500], Loss: 2.5173, MAE: 1.1485, RMSE: 1.5622\n",
            "Epoch [84/1500], Loss: 2.5019, MAE: 1.1435, RMSE: 1.5574\n",
            "Epoch [85/1500], Loss: 2.4866, MAE: 1.1386, RMSE: 1.5526\n",
            "Epoch [86/1500], Loss: 2.4713, MAE: 1.1337, RMSE: 1.5477\n",
            "Epoch [87/1500], Loss: 2.4560, MAE: 1.1288, RMSE: 1.5429\n",
            "Epoch [88/1500], Loss: 2.4408, MAE: 1.1240, RMSE: 1.5381\n",
            "Epoch [89/1500], Loss: 2.4255, MAE: 1.1191, RMSE: 1.5332\n",
            "Epoch [90/1500], Loss: 2.4103, MAE: 1.1143, RMSE: 1.5284\n",
            "Epoch [91/1500], Loss: 2.3951, MAE: 1.1095, RMSE: 1.5235\n",
            "Epoch [92/1500], Loss: 2.3800, MAE: 1.1046, RMSE: 1.5186\n",
            "Epoch [93/1500], Loss: 2.3649, MAE: 1.0998, RMSE: 1.5138\n",
            "Epoch [94/1500], Loss: 2.3498, MAE: 1.0950, RMSE: 1.5089\n",
            "Epoch [95/1500], Loss: 2.3347, MAE: 1.0902, RMSE: 1.5040\n",
            "Epoch [96/1500], Loss: 2.3196, MAE: 1.0854, RMSE: 1.4991\n",
            "Epoch [97/1500], Loss: 2.3046, MAE: 1.0807, RMSE: 1.4942\n",
            "Epoch [98/1500], Loss: 2.2896, MAE: 1.0760, RMSE: 1.4893\n",
            "Epoch [99/1500], Loss: 2.2746, MAE: 1.0713, RMSE: 1.4844\n",
            "Epoch [100/1500], Loss: 2.2597, MAE: 1.0666, RMSE: 1.4795\n",
            "Epoch [101/1500], Loss: 2.2447, MAE: 1.0619, RMSE: 1.4745\n",
            "Epoch [102/1500], Loss: 2.2298, MAE: 1.0573, RMSE: 1.4696\n",
            "Epoch [103/1500], Loss: 2.2150, MAE: 1.0526, RMSE: 1.4647\n",
            "Epoch [104/1500], Loss: 2.2001, MAE: 1.0480, RMSE: 1.4597\n",
            "Epoch [105/1500], Loss: 2.1853, MAE: 1.0434, RMSE: 1.4547\n",
            "Epoch [106/1500], Loss: 2.1705, MAE: 1.0389, RMSE: 1.4498\n",
            "Epoch [107/1500], Loss: 2.1557, MAE: 1.0343, RMSE: 1.4448\n",
            "Epoch [108/1500], Loss: 2.1409, MAE: 1.0298, RMSE: 1.4398\n",
            "Epoch [109/1500], Loss: 2.1262, MAE: 1.0253, RMSE: 1.4348\n",
            "Epoch [110/1500], Loss: 2.1115, MAE: 1.0208, RMSE: 1.4298\n",
            "Epoch [111/1500], Loss: 2.0968, MAE: 1.0163, RMSE: 1.4248\n",
            "Epoch [112/1500], Loss: 2.0822, MAE: 1.0119, RMSE: 1.4198\n",
            "Epoch [113/1500], Loss: 2.0676, MAE: 1.0074, RMSE: 1.4148\n",
            "Epoch [114/1500], Loss: 2.0530, MAE: 1.0030, RMSE: 1.4098\n",
            "Epoch [115/1500], Loss: 2.0384, MAE: 0.9987, RMSE: 1.4048\n",
            "Epoch [116/1500], Loss: 2.0239, MAE: 0.9944, RMSE: 1.3997\n",
            "Epoch [117/1500], Loss: 2.0094, MAE: 0.9901, RMSE: 1.3947\n",
            "Epoch [118/1500], Loss: 1.9950, MAE: 0.9858, RMSE: 1.3896\n",
            "Epoch [119/1500], Loss: 1.9805, MAE: 0.9816, RMSE: 1.3846\n",
            "Epoch [120/1500], Loss: 1.9661, MAE: 0.9773, RMSE: 1.3795\n",
            "Epoch [121/1500], Loss: 1.9517, MAE: 0.9732, RMSE: 1.3744\n",
            "Epoch [122/1500], Loss: 1.9374, MAE: 0.9690, RMSE: 1.3694\n",
            "Epoch [123/1500], Loss: 1.9231, MAE: 0.9648, RMSE: 1.3643\n",
            "Epoch [124/1500], Loss: 1.9088, MAE: 0.9607, RMSE: 1.3593\n",
            "Epoch [125/1500], Loss: 1.8945, MAE: 0.9566, RMSE: 1.3542\n",
            "Epoch [126/1500], Loss: 1.8803, MAE: 0.9526, RMSE: 1.3492\n",
            "Epoch [127/1500], Loss: 1.8661, MAE: 0.9486, RMSE: 1.3441\n",
            "Epoch [128/1500], Loss: 1.8519, MAE: 0.9446, RMSE: 1.3390\n",
            "Epoch [129/1500], Loss: 1.8378, MAE: 0.9406, RMSE: 1.3340\n",
            "Epoch [130/1500], Loss: 1.8237, MAE: 0.9366, RMSE: 1.3289\n",
            "Epoch [131/1500], Loss: 1.8097, MAE: 0.9327, RMSE: 1.3238\n",
            "Epoch [132/1500], Loss: 1.7957, MAE: 0.9288, RMSE: 1.3187\n",
            "Epoch [133/1500], Loss: 1.7817, MAE: 0.9249, RMSE: 1.3136\n",
            "Epoch [134/1500], Loss: 1.7678, MAE: 0.9211, RMSE: 1.3085\n",
            "Epoch [135/1500], Loss: 1.7539, MAE: 0.9172, RMSE: 1.3034\n",
            "Epoch [136/1500], Loss: 1.7400, MAE: 0.9134, RMSE: 1.2984\n",
            "Epoch [137/1500], Loss: 1.7262, MAE: 0.9095, RMSE: 1.2933\n",
            "Epoch [138/1500], Loss: 1.7124, MAE: 0.9057, RMSE: 1.2882\n",
            "Epoch [139/1500], Loss: 1.6987, MAE: 0.9020, RMSE: 1.2831\n",
            "Epoch [140/1500], Loss: 1.6851, MAE: 0.8982, RMSE: 1.2780\n",
            "Epoch [141/1500], Loss: 1.6714, MAE: 0.8945, RMSE: 1.2729\n",
            "Epoch [142/1500], Loss: 1.6579, MAE: 0.8909, RMSE: 1.2678\n",
            "Epoch [143/1500], Loss: 1.6444, MAE: 0.8873, RMSE: 1.2627\n",
            "Epoch [144/1500], Loss: 1.6309, MAE: 0.8837, RMSE: 1.2576\n",
            "Epoch [145/1500], Loss: 1.6175, MAE: 0.8802, RMSE: 1.2525\n",
            "Epoch [146/1500], Loss: 1.6041, MAE: 0.8766, RMSE: 1.2474\n",
            "Epoch [147/1500], Loss: 1.5908, MAE: 0.8732, RMSE: 1.2424\n",
            "Epoch [148/1500], Loss: 1.5776, MAE: 0.8697, RMSE: 1.2373\n",
            "Epoch [149/1500], Loss: 1.5644, MAE: 0.8663, RMSE: 1.2322\n",
            "Epoch [150/1500], Loss: 1.5512, MAE: 0.8630, RMSE: 1.2271\n",
            "Epoch [151/1500], Loss: 1.5381, MAE: 0.8597, RMSE: 1.2221\n",
            "Epoch [152/1500], Loss: 1.5251, MAE: 0.8564, RMSE: 1.2170\n",
            "Epoch [153/1500], Loss: 1.5122, MAE: 0.8531, RMSE: 1.2120\n",
            "Epoch [154/1500], Loss: 1.4993, MAE: 0.8499, RMSE: 1.2070\n",
            "Epoch [155/1500], Loss: 1.4865, MAE: 0.8468, RMSE: 1.2019\n",
            "Epoch [156/1500], Loss: 1.4738, MAE: 0.8436, RMSE: 1.1969\n",
            "Epoch [157/1500], Loss: 1.4611, MAE: 0.8406, RMSE: 1.1919\n",
            "Epoch [158/1500], Loss: 1.4485, MAE: 0.8375, RMSE: 1.1869\n",
            "Epoch [159/1500], Loss: 1.4360, MAE: 0.8345, RMSE: 1.1819\n",
            "Epoch [160/1500], Loss: 1.4236, MAE: 0.8315, RMSE: 1.1769\n",
            "Epoch [161/1500], Loss: 1.4112, MAE: 0.8286, RMSE: 1.1720\n",
            "Epoch [162/1500], Loss: 1.3990, MAE: 0.8257, RMSE: 1.1670\n",
            "Epoch [163/1500], Loss: 1.3868, MAE: 0.8228, RMSE: 1.1621\n",
            "Epoch [164/1500], Loss: 1.3747, MAE: 0.8200, RMSE: 1.1572\n",
            "Epoch [165/1500], Loss: 1.3627, MAE: 0.8172, RMSE: 1.1522\n",
            "Epoch [166/1500], Loss: 1.3507, MAE: 0.8145, RMSE: 1.1473\n",
            "Epoch [167/1500], Loss: 1.3389, MAE: 0.8118, RMSE: 1.1425\n",
            "Epoch [168/1500], Loss: 1.3272, MAE: 0.8092, RMSE: 1.1376\n",
            "Epoch [169/1500], Loss: 1.3155, MAE: 0.8066, RMSE: 1.1328\n",
            "Epoch [170/1500], Loss: 1.3040, MAE: 0.8040, RMSE: 1.1280\n",
            "Epoch [171/1500], Loss: 1.2925, MAE: 0.8015, RMSE: 1.1232\n",
            "Epoch [172/1500], Loss: 1.2812, MAE: 0.7990, RMSE: 1.1184\n",
            "Epoch [173/1500], Loss: 1.2699, MAE: 0.7965, RMSE: 1.1136\n",
            "Epoch [174/1500], Loss: 1.2588, MAE: 0.7940, RMSE: 1.1089\n",
            "Epoch [175/1500], Loss: 1.2478, MAE: 0.7916, RMSE: 1.1042\n",
            "Epoch [176/1500], Loss: 1.2368, MAE: 0.7892, RMSE: 1.0996\n",
            "Epoch [177/1500], Loss: 1.2260, MAE: 0.7869, RMSE: 1.0949\n",
            "Epoch [178/1500], Loss: 1.2153, MAE: 0.7846, RMSE: 1.0903\n",
            "Epoch [179/1500], Loss: 1.2047, MAE: 0.7824, RMSE: 1.0857\n",
            "Epoch [180/1500], Loss: 1.1943, MAE: 0.7801, RMSE: 1.0812\n",
            "Epoch [181/1500], Loss: 1.1839, MAE: 0.7779, RMSE: 1.0767\n",
            "Epoch [182/1500], Loss: 1.1737, MAE: 0.7758, RMSE: 1.0722\n",
            "Epoch [183/1500], Loss: 1.1635, MAE: 0.7737, RMSE: 1.0677\n",
            "Epoch [184/1500], Loss: 1.1536, MAE: 0.7716, RMSE: 1.0633\n",
            "Epoch [185/1500], Loss: 1.1437, MAE: 0.7695, RMSE: 1.0590\n",
            "Epoch [186/1500], Loss: 1.1339, MAE: 0.7675, RMSE: 1.0546\n",
            "Epoch [187/1500], Loss: 1.1243, MAE: 0.7655, RMSE: 1.0503\n",
            "Epoch [188/1500], Loss: 1.1149, MAE: 0.7635, RMSE: 1.0461\n",
            "Epoch [189/1500], Loss: 1.1055, MAE: 0.7616, RMSE: 1.0419\n",
            "Epoch [190/1500], Loss: 1.0963, MAE: 0.7597, RMSE: 1.0377\n",
            "Epoch [191/1500], Loss: 1.0872, MAE: 0.7578, RMSE: 1.0336\n",
            "Epoch [192/1500], Loss: 1.0782, MAE: 0.7560, RMSE: 1.0295\n",
            "Epoch [193/1500], Loss: 1.0694, MAE: 0.7542, RMSE: 1.0255\n",
            "Epoch [194/1500], Loss: 1.0607, MAE: 0.7524, RMSE: 1.0215\n",
            "Epoch [195/1500], Loss: 1.0521, MAE: 0.7506, RMSE: 1.0175\n",
            "Epoch [196/1500], Loss: 1.0437, MAE: 0.7489, RMSE: 1.0136\n",
            "Epoch [197/1500], Loss: 1.0355, MAE: 0.7472, RMSE: 1.0098\n",
            "Epoch [198/1500], Loss: 1.0273, MAE: 0.7456, RMSE: 1.0060\n",
            "Epoch [199/1500], Loss: 1.0193, MAE: 0.7440, RMSE: 1.0022\n",
            "Epoch [200/1500], Loss: 1.0114, MAE: 0.7424, RMSE: 0.9985\n",
            "Epoch [201/1500], Loss: 1.0037, MAE: 0.7409, RMSE: 0.9948\n",
            "Epoch [202/1500], Loss: 0.9961, MAE: 0.7393, RMSE: 0.9912\n",
            "Epoch [203/1500], Loss: 0.9887, MAE: 0.7378, RMSE: 0.9876\n",
            "Epoch [204/1500], Loss: 0.9813, MAE: 0.7362, RMSE: 0.9841\n",
            "Epoch [205/1500], Loss: 0.9742, MAE: 0.7347, RMSE: 0.9806\n",
            "Epoch [206/1500], Loss: 0.9671, MAE: 0.7331, RMSE: 0.9772\n",
            "Epoch [207/1500], Loss: 0.9602, MAE: 0.7316, RMSE: 0.9739\n",
            "Epoch [208/1500], Loss: 0.9535, MAE: 0.7301, RMSE: 0.9705\n",
            "Epoch [209/1500], Loss: 0.9469, MAE: 0.7286, RMSE: 0.9673\n",
            "Epoch [210/1500], Loss: 0.9404, MAE: 0.7271, RMSE: 0.9641\n",
            "Epoch [211/1500], Loss: 0.9340, MAE: 0.7256, RMSE: 0.9609\n",
            "Epoch [212/1500], Loss: 0.9278, MAE: 0.7242, RMSE: 0.9578\n",
            "Epoch [213/1500], Loss: 0.9217, MAE: 0.7227, RMSE: 0.9548\n",
            "Epoch [214/1500], Loss: 0.9157, MAE: 0.7214, RMSE: 0.9518\n",
            "Epoch [215/1500], Loss: 0.9098, MAE: 0.7200, RMSE: 0.9488\n",
            "Epoch [216/1500], Loss: 0.9041, MAE: 0.7187, RMSE: 0.9459\n",
            "Epoch [217/1500], Loss: 0.8985, MAE: 0.7173, RMSE: 0.9431\n",
            "Epoch [218/1500], Loss: 0.8930, MAE: 0.7161, RMSE: 0.9403\n",
            "Epoch [219/1500], Loss: 0.8877, MAE: 0.7148, RMSE: 0.9376\n",
            "Epoch [220/1500], Loss: 0.8825, MAE: 0.7135, RMSE: 0.9349\n",
            "Epoch [221/1500], Loss: 0.8774, MAE: 0.7122, RMSE: 0.9322\n",
            "Epoch [222/1500], Loss: 0.8724, MAE: 0.7110, RMSE: 0.9296\n",
            "Epoch [223/1500], Loss: 0.8675, MAE: 0.7097, RMSE: 0.9271\n",
            "Epoch [224/1500], Loss: 0.8627, MAE: 0.7085, RMSE: 0.9245\n",
            "Epoch [225/1500], Loss: 0.8580, MAE: 0.7073, RMSE: 0.9221\n",
            "Epoch [226/1500], Loss: 0.8535, MAE: 0.7060, RMSE: 0.9197\n",
            "Epoch [227/1500], Loss: 0.8490, MAE: 0.7048, RMSE: 0.9173\n",
            "Epoch [228/1500], Loss: 0.8447, MAE: 0.7036, RMSE: 0.9150\n",
            "Epoch [229/1500], Loss: 0.8404, MAE: 0.7024, RMSE: 0.9127\n",
            "Epoch [230/1500], Loss: 0.8362, MAE: 0.7012, RMSE: 0.9105\n",
            "Epoch [231/1500], Loss: 0.8322, MAE: 0.7000, RMSE: 0.9083\n",
            "Epoch [232/1500], Loss: 0.8282, MAE: 0.6988, RMSE: 0.9062\n",
            "Epoch [233/1500], Loss: 0.8243, MAE: 0.6976, RMSE: 0.9041\n",
            "Epoch [234/1500], Loss: 0.8205, MAE: 0.6964, RMSE: 0.9020\n",
            "Epoch [235/1500], Loss: 0.8168, MAE: 0.6952, RMSE: 0.9000\n",
            "Epoch [236/1500], Loss: 0.8132, MAE: 0.6941, RMSE: 0.8980\n",
            "Epoch [237/1500], Loss: 0.8097, MAE: 0.6929, RMSE: 0.8961\n",
            "Epoch [238/1500], Loss: 0.8062, MAE: 0.6918, RMSE: 0.8942\n",
            "Epoch [239/1500], Loss: 0.8028, MAE: 0.6907, RMSE: 0.8923\n",
            "Epoch [240/1500], Loss: 0.7995, MAE: 0.6896, RMSE: 0.8905\n",
            "Epoch [241/1500], Loss: 0.7962, MAE: 0.6885, RMSE: 0.8887\n",
            "Epoch [242/1500], Loss: 0.7930, MAE: 0.6874, RMSE: 0.8869\n",
            "Epoch [243/1500], Loss: 0.7899, MAE: 0.6863, RMSE: 0.8852\n",
            "Epoch [244/1500], Loss: 0.7868, MAE: 0.6852, RMSE: 0.8835\n",
            "Epoch [245/1500], Loss: 0.7838, MAE: 0.6841, RMSE: 0.8818\n",
            "Epoch [246/1500], Loss: 0.7808, MAE: 0.6830, RMSE: 0.8802\n",
            "Epoch [247/1500], Loss: 0.7780, MAE: 0.6819, RMSE: 0.8786\n",
            "Epoch [248/1500], Loss: 0.7751, MAE: 0.6808, RMSE: 0.8770\n",
            "Epoch [249/1500], Loss: 0.7723, MAE: 0.6797, RMSE: 0.8755\n",
            "Epoch [250/1500], Loss: 0.7696, MAE: 0.6786, RMSE: 0.8740\n",
            "Epoch [251/1500], Loss: 0.7669, MAE: 0.6775, RMSE: 0.8725\n",
            "Epoch [252/1500], Loss: 0.7643, MAE: 0.6764, RMSE: 0.8710\n",
            "Epoch [253/1500], Loss: 0.7617, MAE: 0.6753, RMSE: 0.8696\n",
            "Epoch [254/1500], Loss: 0.7592, MAE: 0.6742, RMSE: 0.8682\n",
            "Epoch [255/1500], Loss: 0.7567, MAE: 0.6731, RMSE: 0.8668\n",
            "Epoch [256/1500], Loss: 0.7542, MAE: 0.6721, RMSE: 0.8655\n",
            "Epoch [257/1500], Loss: 0.7518, MAE: 0.6710, RMSE: 0.8641\n",
            "Epoch [258/1500], Loss: 0.7495, MAE: 0.6699, RMSE: 0.8628\n",
            "Epoch [259/1500], Loss: 0.7471, MAE: 0.6688, RMSE: 0.8616\n",
            "Epoch [260/1500], Loss: 0.7449, MAE: 0.6678, RMSE: 0.8603\n",
            "Epoch [261/1500], Loss: 0.7426, MAE: 0.6667, RMSE: 0.8591\n",
            "Epoch [262/1500], Loss: 0.7404, MAE: 0.6656, RMSE: 0.8579\n",
            "Epoch [263/1500], Loss: 0.7382, MAE: 0.6645, RMSE: 0.8567\n",
            "Epoch [264/1500], Loss: 0.7360, MAE: 0.6635, RMSE: 0.8555\n",
            "Epoch [265/1500], Loss: 0.7339, MAE: 0.6624, RMSE: 0.8543\n",
            "Epoch [266/1500], Loss: 0.7318, MAE: 0.6614, RMSE: 0.8532\n",
            "Epoch [267/1500], Loss: 0.7298, MAE: 0.6603, RMSE: 0.8521\n",
            "Epoch [268/1500], Loss: 0.7277, MAE: 0.6592, RMSE: 0.8510\n",
            "Epoch [269/1500], Loss: 0.7257, MAE: 0.6582, RMSE: 0.8499\n",
            "Epoch [270/1500], Loss: 0.7238, MAE: 0.6572, RMSE: 0.8489\n",
            "Epoch [271/1500], Loss: 0.7218, MAE: 0.6561, RMSE: 0.8478\n",
            "Epoch [272/1500], Loss: 0.7199, MAE: 0.6551, RMSE: 0.8468\n",
            "Epoch [273/1500], Loss: 0.7180, MAE: 0.6541, RMSE: 0.8458\n",
            "Epoch [274/1500], Loss: 0.7162, MAE: 0.6530, RMSE: 0.8448\n",
            "Epoch [275/1500], Loss: 0.7143, MAE: 0.6520, RMSE: 0.8438\n",
            "Epoch [276/1500], Loss: 0.7125, MAE: 0.6510, RMSE: 0.8428\n",
            "Epoch [277/1500], Loss: 0.7107, MAE: 0.6500, RMSE: 0.8419\n",
            "Epoch [278/1500], Loss: 0.7090, MAE: 0.6490, RMSE: 0.8409\n",
            "Epoch [279/1500], Loss: 0.7072, MAE: 0.6480, RMSE: 0.8400\n",
            "Epoch [280/1500], Loss: 0.7055, MAE: 0.6471, RMSE: 0.8391\n",
            "Epoch [281/1500], Loss: 0.7038, MAE: 0.6461, RMSE: 0.8382\n",
            "Epoch [282/1500], Loss: 0.7021, MAE: 0.6452, RMSE: 0.8373\n",
            "Epoch [283/1500], Loss: 0.7005, MAE: 0.6442, RMSE: 0.8364\n",
            "Epoch [284/1500], Loss: 0.6988, MAE: 0.6433, RMSE: 0.8355\n",
            "Epoch [285/1500], Loss: 0.6972, MAE: 0.6424, RMSE: 0.8347\n",
            "Epoch [286/1500], Loss: 0.6956, MAE: 0.6414, RMSE: 0.8338\n",
            "Epoch [287/1500], Loss: 0.6940, MAE: 0.6405, RMSE: 0.8330\n",
            "Epoch [288/1500], Loss: 0.6925, MAE: 0.6396, RMSE: 0.8321\n",
            "Epoch [289/1500], Loss: 0.6909, MAE: 0.6387, RMSE: 0.8313\n",
            "Epoch [290/1500], Loss: 0.6894, MAE: 0.6378, RMSE: 0.8305\n",
            "Epoch [291/1500], Loss: 0.6879, MAE: 0.6369, RMSE: 0.8297\n",
            "Epoch [292/1500], Loss: 0.6864, MAE: 0.6360, RMSE: 0.8289\n",
            "Epoch [293/1500], Loss: 0.6849, MAE: 0.6351, RMSE: 0.8281\n",
            "Epoch [294/1500], Loss: 0.6834, MAE: 0.6343, RMSE: 0.8273\n",
            "Epoch [295/1500], Loss: 0.6820, MAE: 0.6334, RMSE: 0.8266\n",
            "Epoch [296/1500], Loss: 0.6805, MAE: 0.6325, RMSE: 0.8258\n",
            "Epoch [297/1500], Loss: 0.6791, MAE: 0.6317, RMSE: 0.8250\n",
            "Epoch [298/1500], Loss: 0.6777, MAE: 0.6308, RMSE: 0.8243\n",
            "Epoch [299/1500], Loss: 0.6763, MAE: 0.6300, RMSE: 0.8235\n",
            "Epoch [300/1500], Loss: 0.6750, MAE: 0.6291, RMSE: 0.8228\n",
            "Epoch [301/1500], Loss: 0.6736, MAE: 0.6283, RMSE: 0.8221\n",
            "Epoch [302/1500], Loss: 0.6723, MAE: 0.6275, RMSE: 0.8214\n",
            "Epoch [303/1500], Loss: 0.6709, MAE: 0.6267, RMSE: 0.8206\n",
            "Epoch [304/1500], Loss: 0.6696, MAE: 0.6258, RMSE: 0.8199\n",
            "Epoch [305/1500], Loss: 0.6683, MAE: 0.6250, RMSE: 0.8192\n",
            "Epoch [306/1500], Loss: 0.6670, MAE: 0.6242, RMSE: 0.8185\n",
            "Epoch [307/1500], Loss: 0.6657, MAE: 0.6234, RMSE: 0.8178\n",
            "Epoch [308/1500], Loss: 0.6644, MAE: 0.6226, RMSE: 0.8171\n",
            "Epoch [309/1500], Loss: 0.6632, MAE: 0.6218, RMSE: 0.8165\n",
            "Epoch [310/1500], Loss: 0.6619, MAE: 0.6211, RMSE: 0.8158\n",
            "Epoch [311/1500], Loss: 0.6607, MAE: 0.6203, RMSE: 0.8151\n",
            "Epoch [312/1500], Loss: 0.6594, MAE: 0.6195, RMSE: 0.8144\n",
            "Epoch [313/1500], Loss: 0.6582, MAE: 0.6187, RMSE: 0.8138\n",
            "Epoch [314/1500], Loss: 0.6570, MAE: 0.6179, RMSE: 0.8131\n",
            "Epoch [315/1500], Loss: 0.6558, MAE: 0.6172, RMSE: 0.8125\n",
            "Epoch [316/1500], Loss: 0.6546, MAE: 0.6164, RMSE: 0.8118\n",
            "Epoch [317/1500], Loss: 0.6534, MAE: 0.6157, RMSE: 0.8112\n",
            "Epoch [318/1500], Loss: 0.6522, MAE: 0.6149, RMSE: 0.8105\n",
            "Epoch [319/1500], Loss: 0.6511, MAE: 0.6141, RMSE: 0.8099\n",
            "Epoch [320/1500], Loss: 0.6499, MAE: 0.6134, RMSE: 0.8092\n",
            "Epoch [321/1500], Loss: 0.6488, MAE: 0.6126, RMSE: 0.8086\n",
            "Epoch [322/1500], Loss: 0.6476, MAE: 0.6119, RMSE: 0.8080\n",
            "Epoch [323/1500], Loss: 0.6465, MAE: 0.6112, RMSE: 0.8073\n",
            "Epoch [324/1500], Loss: 0.6454, MAE: 0.6104, RMSE: 0.8067\n",
            "Epoch [325/1500], Loss: 0.6443, MAE: 0.6097, RMSE: 0.8061\n",
            "Epoch [326/1500], Loss: 0.6432, MAE: 0.6090, RMSE: 0.8055\n",
            "Epoch [327/1500], Loss: 0.6421, MAE: 0.6083, RMSE: 0.8049\n",
            "Epoch [328/1500], Loss: 0.6410, MAE: 0.6075, RMSE: 0.8042\n",
            "Epoch [329/1500], Loss: 0.6399, MAE: 0.6068, RMSE: 0.8036\n",
            "Epoch [330/1500], Loss: 0.6388, MAE: 0.6061, RMSE: 0.8030\n",
            "Epoch [331/1500], Loss: 0.6378, MAE: 0.6054, RMSE: 0.8024\n",
            "Epoch [332/1500], Loss: 0.6367, MAE: 0.6047, RMSE: 0.8018\n",
            "Epoch [333/1500], Loss: 0.6357, MAE: 0.6040, RMSE: 0.8012\n",
            "Epoch [334/1500], Loss: 0.6346, MAE: 0.6034, RMSE: 0.8006\n",
            "Epoch [335/1500], Loss: 0.6336, MAE: 0.6027, RMSE: 0.8000\n",
            "Epoch [336/1500], Loss: 0.6326, MAE: 0.6020, RMSE: 0.7995\n",
            "Epoch [337/1500], Loss: 0.6315, MAE: 0.6013, RMSE: 0.7989\n",
            "Epoch [338/1500], Loss: 0.6305, MAE: 0.6006, RMSE: 0.7983\n",
            "Epoch [339/1500], Loss: 0.6295, MAE: 0.6000, RMSE: 0.7977\n",
            "Epoch [340/1500], Loss: 0.6285, MAE: 0.5993, RMSE: 0.7971\n",
            "Epoch [341/1500], Loss: 0.6275, MAE: 0.5986, RMSE: 0.7966\n",
            "Epoch [342/1500], Loss: 0.6265, MAE: 0.5980, RMSE: 0.7960\n",
            "Epoch [343/1500], Loss: 0.6255, MAE: 0.5973, RMSE: 0.7954\n",
            "Epoch [344/1500], Loss: 0.6245, MAE: 0.5967, RMSE: 0.7948\n",
            "Epoch [345/1500], Loss: 0.6235, MAE: 0.5960, RMSE: 0.7943\n",
            "Epoch [346/1500], Loss: 0.6225, MAE: 0.5954, RMSE: 0.7937\n",
            "Epoch [347/1500], Loss: 0.6216, MAE: 0.5947, RMSE: 0.7932\n",
            "Epoch [348/1500], Loss: 0.6206, MAE: 0.5941, RMSE: 0.7926\n",
            "Epoch [349/1500], Loss: 0.6196, MAE: 0.5934, RMSE: 0.7920\n",
            "Epoch [350/1500], Loss: 0.6187, MAE: 0.5928, RMSE: 0.7915\n",
            "Epoch [351/1500], Loss: 0.6177, MAE: 0.5922, RMSE: 0.7909\n",
            "Epoch [352/1500], Loss: 0.6168, MAE: 0.5915, RMSE: 0.7904\n",
            "Epoch [353/1500], Loss: 0.6158, MAE: 0.5909, RMSE: 0.7898\n",
            "Epoch [354/1500], Loss: 0.6149, MAE: 0.5903, RMSE: 0.7893\n",
            "Epoch [355/1500], Loss: 0.6140, MAE: 0.5897, RMSE: 0.7887\n",
            "Epoch [356/1500], Loss: 0.6131, MAE: 0.5890, RMSE: 0.7882\n",
            "Epoch [357/1500], Loss: 0.6121, MAE: 0.5884, RMSE: 0.7876\n",
            "Epoch [358/1500], Loss: 0.6112, MAE: 0.5878, RMSE: 0.7871\n",
            "Epoch [359/1500], Loss: 0.6103, MAE: 0.5872, RMSE: 0.7865\n",
            "Epoch [360/1500], Loss: 0.6094, MAE: 0.5866, RMSE: 0.7860\n",
            "Epoch [361/1500], Loss: 0.6085, MAE: 0.5860, RMSE: 0.7855\n",
            "Epoch [362/1500], Loss: 0.6076, MAE: 0.5854, RMSE: 0.7849\n",
            "Epoch [363/1500], Loss: 0.6067, MAE: 0.5848, RMSE: 0.7844\n",
            "Epoch [364/1500], Loss: 0.6058, MAE: 0.5842, RMSE: 0.7839\n",
            "Epoch [365/1500], Loss: 0.6049, MAE: 0.5836, RMSE: 0.7833\n",
            "Epoch [366/1500], Loss: 0.6040, MAE: 0.5830, RMSE: 0.7828\n",
            "Epoch [367/1500], Loss: 0.6031, MAE: 0.5824, RMSE: 0.7823\n",
            "Epoch [368/1500], Loss: 0.6022, MAE: 0.5818, RMSE: 0.7818\n",
            "Epoch [369/1500], Loss: 0.6014, MAE: 0.5812, RMSE: 0.7812\n",
            "Epoch [370/1500], Loss: 0.6005, MAE: 0.5806, RMSE: 0.7807\n",
            "Epoch [371/1500], Loss: 0.5996, MAE: 0.5800, RMSE: 0.7802\n",
            "Epoch [372/1500], Loss: 0.5987, MAE: 0.5795, RMSE: 0.7797\n",
            "Epoch [373/1500], Loss: 0.5979, MAE: 0.5789, RMSE: 0.7792\n",
            "Epoch [374/1500], Loss: 0.5970, MAE: 0.5783, RMSE: 0.7786\n",
            "Epoch [375/1500], Loss: 0.5962, MAE: 0.5777, RMSE: 0.7781\n",
            "Epoch [376/1500], Loss: 0.5953, MAE: 0.5771, RMSE: 0.7776\n",
            "Epoch [377/1500], Loss: 0.5944, MAE: 0.5766, RMSE: 0.7771\n",
            "Epoch [378/1500], Loss: 0.5936, MAE: 0.5760, RMSE: 0.7766\n",
            "Epoch [379/1500], Loss: 0.5927, MAE: 0.5754, RMSE: 0.7761\n",
            "Epoch [380/1500], Loss: 0.5919, MAE: 0.5749, RMSE: 0.7756\n",
            "Epoch [381/1500], Loss: 0.5911, MAE: 0.5743, RMSE: 0.7750\n",
            "Epoch [382/1500], Loss: 0.5902, MAE: 0.5737, RMSE: 0.7745\n",
            "Epoch [383/1500], Loss: 0.5894, MAE: 0.5732, RMSE: 0.7740\n",
            "Epoch [384/1500], Loss: 0.5885, MAE: 0.5726, RMSE: 0.7735\n",
            "Epoch [385/1500], Loss: 0.5877, MAE: 0.5721, RMSE: 0.7730\n",
            "Epoch [386/1500], Loss: 0.5869, MAE: 0.5715, RMSE: 0.7725\n",
            "Epoch [387/1500], Loss: 0.5860, MAE: 0.5710, RMSE: 0.7720\n",
            "Epoch [388/1500], Loss: 0.5852, MAE: 0.5704, RMSE: 0.7715\n",
            "Epoch [389/1500], Loss: 0.5844, MAE: 0.5699, RMSE: 0.7710\n",
            "Epoch [390/1500], Loss: 0.5836, MAE: 0.5694, RMSE: 0.7705\n",
            "Epoch [391/1500], Loss: 0.5828, MAE: 0.5689, RMSE: 0.7700\n",
            "Epoch [392/1500], Loss: 0.5819, MAE: 0.5683, RMSE: 0.7695\n",
            "Epoch [393/1500], Loss: 0.5811, MAE: 0.5678, RMSE: 0.7691\n",
            "Epoch [394/1500], Loss: 0.5803, MAE: 0.5673, RMSE: 0.7686\n",
            "Epoch [395/1500], Loss: 0.5795, MAE: 0.5668, RMSE: 0.7681\n",
            "Epoch [396/1500], Loss: 0.5787, MAE: 0.5663, RMSE: 0.7676\n",
            "Epoch [397/1500], Loss: 0.5779, MAE: 0.5657, RMSE: 0.7671\n",
            "Epoch [398/1500], Loss: 0.5771, MAE: 0.5652, RMSE: 0.7666\n",
            "Epoch [399/1500], Loss: 0.5763, MAE: 0.5647, RMSE: 0.7661\n",
            "Epoch [400/1500], Loss: 0.5755, MAE: 0.5642, RMSE: 0.7656\n",
            "Epoch [401/1500], Loss: 0.5747, MAE: 0.5637, RMSE: 0.7652\n",
            "Epoch [402/1500], Loss: 0.5739, MAE: 0.5632, RMSE: 0.7647\n",
            "Epoch [403/1500], Loss: 0.5731, MAE: 0.5627, RMSE: 0.7642\n",
            "Epoch [404/1500], Loss: 0.5723, MAE: 0.5622, RMSE: 0.7637\n",
            "Epoch [405/1500], Loss: 0.5715, MAE: 0.5617, RMSE: 0.7632\n",
            "Epoch [406/1500], Loss: 0.5707, MAE: 0.5612, RMSE: 0.7628\n",
            "Epoch [407/1500], Loss: 0.5699, MAE: 0.5607, RMSE: 0.7623\n",
            "Epoch [408/1500], Loss: 0.5692, MAE: 0.5602, RMSE: 0.7618\n",
            "Epoch [409/1500], Loss: 0.5684, MAE: 0.5597, RMSE: 0.7614\n",
            "Epoch [410/1500], Loss: 0.5676, MAE: 0.5592, RMSE: 0.7609\n",
            "Epoch [411/1500], Loss: 0.5668, MAE: 0.5587, RMSE: 0.7604\n",
            "Epoch [412/1500], Loss: 0.5661, MAE: 0.5582, RMSE: 0.7599\n",
            "Epoch [413/1500], Loss: 0.5653, MAE: 0.5577, RMSE: 0.7595\n",
            "Epoch [414/1500], Loss: 0.5645, MAE: 0.5572, RMSE: 0.7590\n",
            "Epoch [415/1500], Loss: 0.5638, MAE: 0.5567, RMSE: 0.7586\n",
            "Epoch [416/1500], Loss: 0.5630, MAE: 0.5562, RMSE: 0.7581\n",
            "Epoch [417/1500], Loss: 0.5622, MAE: 0.5558, RMSE: 0.7576\n",
            "Epoch [418/1500], Loss: 0.5615, MAE: 0.5553, RMSE: 0.7572\n",
            "Epoch [419/1500], Loss: 0.5607, MAE: 0.5548, RMSE: 0.7567\n",
            "Epoch [420/1500], Loss: 0.5600, MAE: 0.5543, RMSE: 0.7563\n",
            "Epoch [421/1500], Loss: 0.5592, MAE: 0.5538, RMSE: 0.7558\n",
            "Epoch [422/1500], Loss: 0.5585, MAE: 0.5534, RMSE: 0.7554\n",
            "Epoch [423/1500], Loss: 0.5577, MAE: 0.5529, RMSE: 0.7549\n",
            "Epoch [424/1500], Loss: 0.5570, MAE: 0.5524, RMSE: 0.7545\n",
            "Epoch [425/1500], Loss: 0.5562, MAE: 0.5520, RMSE: 0.7540\n",
            "Epoch [426/1500], Loss: 0.5555, MAE: 0.5515, RMSE: 0.7536\n",
            "Epoch [427/1500], Loss: 0.5548, MAE: 0.5510, RMSE: 0.7531\n",
            "Epoch [428/1500], Loss: 0.5540, MAE: 0.5506, RMSE: 0.7527\n",
            "Epoch [429/1500], Loss: 0.5533, MAE: 0.5501, RMSE: 0.7522\n",
            "Epoch [430/1500], Loss: 0.5526, MAE: 0.5497, RMSE: 0.7518\n",
            "Epoch [431/1500], Loss: 0.5518, MAE: 0.5492, RMSE: 0.7513\n",
            "Epoch [432/1500], Loss: 0.5511, MAE: 0.5488, RMSE: 0.7509\n",
            "Epoch [433/1500], Loss: 0.5504, MAE: 0.5483, RMSE: 0.7505\n",
            "Epoch [434/1500], Loss: 0.5497, MAE: 0.5479, RMSE: 0.7500\n",
            "Epoch [435/1500], Loss: 0.5490, MAE: 0.5474, RMSE: 0.7496\n",
            "Epoch [436/1500], Loss: 0.5483, MAE: 0.5470, RMSE: 0.7492\n",
            "Epoch [437/1500], Loss: 0.5475, MAE: 0.5465, RMSE: 0.7487\n",
            "Epoch [438/1500], Loss: 0.5468, MAE: 0.5461, RMSE: 0.7483\n",
            "Epoch [439/1500], Loss: 0.5461, MAE: 0.5457, RMSE: 0.7479\n",
            "Epoch [440/1500], Loss: 0.5454, MAE: 0.5452, RMSE: 0.7475\n",
            "Epoch [441/1500], Loss: 0.5447, MAE: 0.5448, RMSE: 0.7470\n",
            "Epoch [442/1500], Loss: 0.5440, MAE: 0.5444, RMSE: 0.7466\n",
            "Epoch [443/1500], Loss: 0.5433, MAE: 0.5440, RMSE: 0.7462\n",
            "Epoch [444/1500], Loss: 0.5426, MAE: 0.5435, RMSE: 0.7458\n",
            "Epoch [445/1500], Loss: 0.5420, MAE: 0.5431, RMSE: 0.7453\n",
            "Epoch [446/1500], Loss: 0.5413, MAE: 0.5427, RMSE: 0.7449\n",
            "Epoch [447/1500], Loss: 0.5406, MAE: 0.5423, RMSE: 0.7445\n",
            "Epoch [448/1500], Loss: 0.5399, MAE: 0.5418, RMSE: 0.7441\n",
            "Epoch [449/1500], Loss: 0.5392, MAE: 0.5414, RMSE: 0.7437\n",
            "Epoch [450/1500], Loss: 0.5385, MAE: 0.5410, RMSE: 0.7433\n",
            "Epoch [451/1500], Loss: 0.5379, MAE: 0.5406, RMSE: 0.7428\n",
            "Epoch [452/1500], Loss: 0.5372, MAE: 0.5402, RMSE: 0.7424\n",
            "Epoch [453/1500], Loss: 0.5365, MAE: 0.5398, RMSE: 0.7420\n",
            "Epoch [454/1500], Loss: 0.5359, MAE: 0.5394, RMSE: 0.7416\n",
            "Epoch [455/1500], Loss: 0.5352, MAE: 0.5390, RMSE: 0.7412\n",
            "Epoch [456/1500], Loss: 0.5345, MAE: 0.5386, RMSE: 0.7408\n",
            "Epoch [457/1500], Loss: 0.5339, MAE: 0.5382, RMSE: 0.7404\n",
            "Epoch [458/1500], Loss: 0.5332, MAE: 0.5378, RMSE: 0.7400\n",
            "Epoch [459/1500], Loss: 0.5326, MAE: 0.5374, RMSE: 0.7396\n",
            "Epoch [460/1500], Loss: 0.5319, MAE: 0.5370, RMSE: 0.7392\n",
            "Epoch [461/1500], Loss: 0.5312, MAE: 0.5366, RMSE: 0.7388\n",
            "Epoch [462/1500], Loss: 0.5306, MAE: 0.5362, RMSE: 0.7385\n",
            "Epoch [463/1500], Loss: 0.5300, MAE: 0.5358, RMSE: 0.7381\n",
            "Epoch [464/1500], Loss: 0.5293, MAE: 0.5354, RMSE: 0.7377\n",
            "Epoch [465/1500], Loss: 0.5287, MAE: 0.5350, RMSE: 0.7373\n",
            "Epoch [466/1500], Loss: 0.5280, MAE: 0.5346, RMSE: 0.7369\n",
            "Epoch [467/1500], Loss: 0.5274, MAE: 0.5343, RMSE: 0.7366\n",
            "Epoch [468/1500], Loss: 0.5268, MAE: 0.5339, RMSE: 0.7362\n",
            "Epoch [469/1500], Loss: 0.5262, MAE: 0.5335, RMSE: 0.7358\n",
            "Epoch [470/1500], Loss: 0.5255, MAE: 0.5331, RMSE: 0.7354\n",
            "Epoch [471/1500], Loss: 0.5249, MAE: 0.5327, RMSE: 0.7351\n",
            "Epoch [472/1500], Loss: 0.5243, MAE: 0.5324, RMSE: 0.7347\n",
            "Epoch [473/1500], Loss: 0.5237, MAE: 0.5320, RMSE: 0.7343\n",
            "Epoch [474/1500], Loss: 0.5231, MAE: 0.5316, RMSE: 0.7339\n",
            "Epoch [475/1500], Loss: 0.5225, MAE: 0.5312, RMSE: 0.7336\n",
            "Epoch [476/1500], Loss: 0.5219, MAE: 0.5309, RMSE: 0.7332\n",
            "Epoch [477/1500], Loss: 0.5213, MAE: 0.5305, RMSE: 0.7328\n",
            "Epoch [478/1500], Loss: 0.5206, MAE: 0.5301, RMSE: 0.7325\n",
            "Epoch [479/1500], Loss: 0.5200, MAE: 0.5298, RMSE: 0.7321\n",
            "Epoch [480/1500], Loss: 0.5194, MAE: 0.5294, RMSE: 0.7318\n",
            "Epoch [481/1500], Loss: 0.5188, MAE: 0.5291, RMSE: 0.7314\n",
            "Epoch [482/1500], Loss: 0.5183, MAE: 0.5287, RMSE: 0.7311\n",
            "Epoch [483/1500], Loss: 0.5177, MAE: 0.5283, RMSE: 0.7307\n",
            "Epoch [484/1500], Loss: 0.5171, MAE: 0.5280, RMSE: 0.7304\n",
            "Epoch [485/1500], Loss: 0.5165, MAE: 0.5276, RMSE: 0.7300\n",
            "Epoch [486/1500], Loss: 0.5159, MAE: 0.5273, RMSE: 0.7297\n",
            "Epoch [487/1500], Loss: 0.5153, MAE: 0.5269, RMSE: 0.7293\n",
            "Epoch [488/1500], Loss: 0.5147, MAE: 0.5266, RMSE: 0.7290\n",
            "Epoch [489/1500], Loss: 0.5142, MAE: 0.5262, RMSE: 0.7286\n",
            "Epoch [490/1500], Loss: 0.5136, MAE: 0.5259, RMSE: 0.7283\n",
            "Epoch [491/1500], Loss: 0.5130, MAE: 0.5256, RMSE: 0.7279\n",
            "Epoch [492/1500], Loss: 0.5124, MAE: 0.5252, RMSE: 0.7276\n",
            "Epoch [493/1500], Loss: 0.5119, MAE: 0.5249, RMSE: 0.7273\n",
            "Epoch [494/1500], Loss: 0.5113, MAE: 0.5245, RMSE: 0.7269\n",
            "Epoch [495/1500], Loss: 0.5108, MAE: 0.5242, RMSE: 0.7266\n",
            "Epoch [496/1500], Loss: 0.5102, MAE: 0.5239, RMSE: 0.7263\n",
            "Epoch [497/1500], Loss: 0.5097, MAE: 0.5236, RMSE: 0.7259\n",
            "Epoch [498/1500], Loss: 0.5091, MAE: 0.5232, RMSE: 0.7256\n",
            "Epoch [499/1500], Loss: 0.5086, MAE: 0.5229, RMSE: 0.7253\n",
            "Epoch [500/1500], Loss: 0.5080, MAE: 0.5226, RMSE: 0.7250\n",
            "Epoch [501/1500], Loss: 0.5075, MAE: 0.5223, RMSE: 0.7247\n",
            "Epoch [502/1500], Loss: 0.5069, MAE: 0.5220, RMSE: 0.7243\n",
            "Epoch [503/1500], Loss: 0.5064, MAE: 0.5217, RMSE: 0.7240\n",
            "Epoch [504/1500], Loss: 0.5059, MAE: 0.5214, RMSE: 0.7237\n",
            "Epoch [505/1500], Loss: 0.5053, MAE: 0.5211, RMSE: 0.7234\n",
            "Epoch [506/1500], Loss: 0.5048, MAE: 0.5208, RMSE: 0.7231\n",
            "Epoch [507/1500], Loss: 0.5043, MAE: 0.5204, RMSE: 0.7228\n",
            "Epoch [508/1500], Loss: 0.5037, MAE: 0.5201, RMSE: 0.7225\n",
            "Epoch [509/1500], Loss: 0.5032, MAE: 0.5198, RMSE: 0.7222\n",
            "Epoch [510/1500], Loss: 0.5027, MAE: 0.5195, RMSE: 0.7219\n",
            "Epoch [511/1500], Loss: 0.5022, MAE: 0.5193, RMSE: 0.7216\n",
            "Epoch [512/1500], Loss: 0.5017, MAE: 0.5190, RMSE: 0.7213\n",
            "Epoch [513/1500], Loss: 0.5012, MAE: 0.5187, RMSE: 0.7210\n",
            "Epoch [514/1500], Loss: 0.5007, MAE: 0.5184, RMSE: 0.7207\n",
            "Epoch [515/1500], Loss: 0.5002, MAE: 0.5181, RMSE: 0.7204\n",
            "Epoch [516/1500], Loss: 0.4997, MAE: 0.5178, RMSE: 0.7201\n",
            "Epoch [517/1500], Loss: 0.4992, MAE: 0.5175, RMSE: 0.7198\n",
            "Epoch [518/1500], Loss: 0.4987, MAE: 0.5173, RMSE: 0.7195\n",
            "Epoch [519/1500], Loss: 0.4982, MAE: 0.5170, RMSE: 0.7192\n",
            "Epoch [520/1500], Loss: 0.4977, MAE: 0.5167, RMSE: 0.7190\n",
            "Epoch [521/1500], Loss: 0.4972, MAE: 0.5164, RMSE: 0.7187\n",
            "Epoch [522/1500], Loss: 0.4968, MAE: 0.5162, RMSE: 0.7184\n",
            "Epoch [523/1500], Loss: 0.4963, MAE: 0.5159, RMSE: 0.7181\n",
            "Epoch [524/1500], Loss: 0.4958, MAE: 0.5157, RMSE: 0.7179\n",
            "Epoch [525/1500], Loss: 0.4953, MAE: 0.5154, RMSE: 0.7176\n",
            "Epoch [526/1500], Loss: 0.4949, MAE: 0.5152, RMSE: 0.7173\n",
            "Epoch [527/1500], Loss: 0.4944, MAE: 0.5149, RMSE: 0.7170\n",
            "Epoch [528/1500], Loss: 0.4939, MAE: 0.5147, RMSE: 0.7167\n",
            "Epoch [529/1500], Loss: 0.4935, MAE: 0.5144, RMSE: 0.7164\n",
            "Epoch [530/1500], Loss: 0.4930, MAE: 0.5142, RMSE: 0.7161\n",
            "Epoch [531/1500], Loss: 0.4926, MAE: 0.5139, RMSE: 0.7159\n",
            "Epoch [532/1500], Loss: 0.4921, MAE: 0.5137, RMSE: 0.7156\n",
            "Epoch [533/1500], Loss: 0.4917, MAE: 0.5134, RMSE: 0.7153\n",
            "Epoch [534/1500], Loss: 0.4912, MAE: 0.5132, RMSE: 0.7150\n",
            "Epoch [535/1500], Loss: 0.4908, MAE: 0.5130, RMSE: 0.7147\n",
            "Epoch [536/1500], Loss: 0.4903, MAE: 0.5127, RMSE: 0.7144\n",
            "Epoch [537/1500], Loss: 0.4899, MAE: 0.5125, RMSE: 0.7142\n",
            "Epoch [538/1500], Loss: 0.4895, MAE: 0.5123, RMSE: 0.7139\n",
            "Epoch [539/1500], Loss: 0.4890, MAE: 0.5120, RMSE: 0.7136\n",
            "Epoch [540/1500], Loss: 0.4886, MAE: 0.5118, RMSE: 0.7133\n",
            "Epoch [541/1500], Loss: 0.4882, MAE: 0.5116, RMSE: 0.7131\n",
            "Epoch [542/1500], Loss: 0.4878, MAE: 0.5114, RMSE: 0.7128\n",
            "Epoch [543/1500], Loss: 0.4873, MAE: 0.5111, RMSE: 0.7125\n",
            "Epoch [544/1500], Loss: 0.4869, MAE: 0.5109, RMSE: 0.7123\n",
            "Epoch [545/1500], Loss: 0.4865, MAE: 0.5107, RMSE: 0.7120\n",
            "Epoch [546/1500], Loss: 0.4861, MAE: 0.5105, RMSE: 0.7117\n",
            "Epoch [547/1500], Loss: 0.4857, MAE: 0.5103, RMSE: 0.7115\n",
            "Epoch [548/1500], Loss: 0.4853, MAE: 0.5100, RMSE: 0.7112\n",
            "Epoch [549/1500], Loss: 0.4849, MAE: 0.5098, RMSE: 0.7109\n",
            "Epoch [550/1500], Loss: 0.4845, MAE: 0.5096, RMSE: 0.7107\n",
            "Epoch [551/1500], Loss: 0.4841, MAE: 0.5094, RMSE: 0.7104\n",
            "Epoch [552/1500], Loss: 0.4837, MAE: 0.5092, RMSE: 0.7101\n",
            "Epoch [553/1500], Loss: 0.4833, MAE: 0.5090, RMSE: 0.7099\n",
            "Epoch [554/1500], Loss: 0.4829, MAE: 0.5088, RMSE: 0.7096\n",
            "Epoch [555/1500], Loss: 0.4825, MAE: 0.5086, RMSE: 0.7094\n",
            "Epoch [556/1500], Loss: 0.4821, MAE: 0.5084, RMSE: 0.7091\n",
            "Epoch [557/1500], Loss: 0.4817, MAE: 0.5082, RMSE: 0.7089\n",
            "Epoch [558/1500], Loss: 0.4813, MAE: 0.5080, RMSE: 0.7086\n",
            "Epoch [559/1500], Loss: 0.4809, MAE: 0.5078, RMSE: 0.7083\n",
            "Epoch [560/1500], Loss: 0.4806, MAE: 0.5076, RMSE: 0.7081\n",
            "Epoch [561/1500], Loss: 0.4802, MAE: 0.5074, RMSE: 0.7078\n",
            "Epoch [562/1500], Loss: 0.4798, MAE: 0.5072, RMSE: 0.7076\n",
            "Epoch [563/1500], Loss: 0.4794, MAE: 0.5070, RMSE: 0.7073\n",
            "Epoch [564/1500], Loss: 0.4791, MAE: 0.5068, RMSE: 0.7071\n",
            "Epoch [565/1500], Loss: 0.4787, MAE: 0.5066, RMSE: 0.7069\n",
            "Epoch [566/1500], Loss: 0.4783, MAE: 0.5064, RMSE: 0.7066\n",
            "Epoch [567/1500], Loss: 0.4780, MAE: 0.5062, RMSE: 0.7064\n",
            "Epoch [568/1500], Loss: 0.4776, MAE: 0.5060, RMSE: 0.7061\n",
            "Epoch [569/1500], Loss: 0.4773, MAE: 0.5058, RMSE: 0.7059\n",
            "Epoch [570/1500], Loss: 0.4769, MAE: 0.5057, RMSE: 0.7056\n",
            "Epoch [571/1500], Loss: 0.4766, MAE: 0.5055, RMSE: 0.7054\n",
            "Epoch [572/1500], Loss: 0.4762, MAE: 0.5053, RMSE: 0.7052\n",
            "Epoch [573/1500], Loss: 0.4759, MAE: 0.5051, RMSE: 0.7049\n",
            "Epoch [574/1500], Loss: 0.4755, MAE: 0.5049, RMSE: 0.7047\n",
            "Epoch [575/1500], Loss: 0.4752, MAE: 0.5048, RMSE: 0.7045\n",
            "Epoch [576/1500], Loss: 0.4748, MAE: 0.5046, RMSE: 0.7042\n",
            "Epoch [577/1500], Loss: 0.4745, MAE: 0.5044, RMSE: 0.7040\n",
            "Epoch [578/1500], Loss: 0.4741, MAE: 0.5042, RMSE: 0.7038\n",
            "Epoch [579/1500], Loss: 0.4738, MAE: 0.5041, RMSE: 0.7035\n",
            "Epoch [580/1500], Loss: 0.4735, MAE: 0.5039, RMSE: 0.7033\n",
            "Epoch [581/1500], Loss: 0.4731, MAE: 0.5037, RMSE: 0.7031\n",
            "Epoch [582/1500], Loss: 0.4728, MAE: 0.5035, RMSE: 0.7028\n",
            "Epoch [583/1500], Loss: 0.4725, MAE: 0.5034, RMSE: 0.7026\n",
            "Epoch [584/1500], Loss: 0.4722, MAE: 0.5032, RMSE: 0.7024\n",
            "Epoch [585/1500], Loss: 0.4718, MAE: 0.5030, RMSE: 0.7022\n",
            "Epoch [586/1500], Loss: 0.4715, MAE: 0.5029, RMSE: 0.7019\n",
            "Epoch [587/1500], Loss: 0.4712, MAE: 0.5027, RMSE: 0.7017\n",
            "Epoch [588/1500], Loss: 0.4709, MAE: 0.5025, RMSE: 0.7015\n",
            "Epoch [589/1500], Loss: 0.4706, MAE: 0.5024, RMSE: 0.7013\n",
            "Epoch [590/1500], Loss: 0.4703, MAE: 0.5022, RMSE: 0.7010\n",
            "Epoch [591/1500], Loss: 0.4700, MAE: 0.5021, RMSE: 0.7008\n",
            "Epoch [592/1500], Loss: 0.4697, MAE: 0.5019, RMSE: 0.7006\n",
            "Epoch [593/1500], Loss: 0.4694, MAE: 0.5017, RMSE: 0.7004\n",
            "Epoch [594/1500], Loss: 0.4691, MAE: 0.5016, RMSE: 0.7001\n",
            "Epoch [595/1500], Loss: 0.4688, MAE: 0.5014, RMSE: 0.6999\n",
            "Epoch [596/1500], Loss: 0.4685, MAE: 0.5012, RMSE: 0.6997\n",
            "Epoch [597/1500], Loss: 0.4682, MAE: 0.5011, RMSE: 0.6995\n",
            "Epoch [598/1500], Loss: 0.4679, MAE: 0.5009, RMSE: 0.6993\n",
            "Epoch [599/1500], Loss: 0.4676, MAE: 0.5008, RMSE: 0.6991\n",
            "Epoch [600/1500], Loss: 0.4673, MAE: 0.5006, RMSE: 0.6988\n",
            "Epoch [601/1500], Loss: 0.4670, MAE: 0.5005, RMSE: 0.6986\n",
            "Epoch [602/1500], Loss: 0.4667, MAE: 0.5003, RMSE: 0.6984\n",
            "Epoch [603/1500], Loss: 0.4664, MAE: 0.5001, RMSE: 0.6982\n",
            "Epoch [604/1500], Loss: 0.4662, MAE: 0.5000, RMSE: 0.6980\n",
            "Epoch [605/1500], Loss: 0.4659, MAE: 0.4998, RMSE: 0.6978\n",
            "Epoch [606/1500], Loss: 0.4656, MAE: 0.4997, RMSE: 0.6976\n",
            "Epoch [607/1500], Loss: 0.4653, MAE: 0.4995, RMSE: 0.6974\n",
            "Epoch [608/1500], Loss: 0.4651, MAE: 0.4994, RMSE: 0.6972\n",
            "Epoch [609/1500], Loss: 0.4648, MAE: 0.4992, RMSE: 0.6970\n",
            "Epoch [610/1500], Loss: 0.4645, MAE: 0.4991, RMSE: 0.6968\n",
            "Epoch [611/1500], Loss: 0.4643, MAE: 0.4990, RMSE: 0.6966\n",
            "Epoch [612/1500], Loss: 0.4640, MAE: 0.4988, RMSE: 0.6964\n",
            "Epoch [613/1500], Loss: 0.4637, MAE: 0.4987, RMSE: 0.6962\n",
            "Epoch [614/1500], Loss: 0.4635, MAE: 0.4985, RMSE: 0.6960\n",
            "Epoch [615/1500], Loss: 0.4632, MAE: 0.4984, RMSE: 0.6958\n",
            "Epoch [616/1500], Loss: 0.4630, MAE: 0.4983, RMSE: 0.6956\n",
            "Epoch [617/1500], Loss: 0.4627, MAE: 0.4981, RMSE: 0.6954\n",
            "Epoch [618/1500], Loss: 0.4625, MAE: 0.4980, RMSE: 0.6952\n",
            "Epoch [619/1500], Loss: 0.4622, MAE: 0.4979, RMSE: 0.6951\n",
            "Epoch [620/1500], Loss: 0.4620, MAE: 0.4977, RMSE: 0.6949\n",
            "Epoch [621/1500], Loss: 0.4617, MAE: 0.4976, RMSE: 0.6947\n",
            "Epoch [622/1500], Loss: 0.4615, MAE: 0.4975, RMSE: 0.6945\n",
            "Epoch [623/1500], Loss: 0.4613, MAE: 0.4973, RMSE: 0.6943\n",
            "Epoch [624/1500], Loss: 0.4610, MAE: 0.4972, RMSE: 0.6941\n",
            "Epoch [625/1500], Loss: 0.4608, MAE: 0.4971, RMSE: 0.6940\n",
            "Epoch [626/1500], Loss: 0.4605, MAE: 0.4970, RMSE: 0.6938\n",
            "Epoch [627/1500], Loss: 0.4603, MAE: 0.4968, RMSE: 0.6936\n",
            "Epoch [628/1500], Loss: 0.4601, MAE: 0.4967, RMSE: 0.6934\n",
            "Epoch [629/1500], Loss: 0.4598, MAE: 0.4966, RMSE: 0.6933\n",
            "Epoch [630/1500], Loss: 0.4596, MAE: 0.4965, RMSE: 0.6931\n",
            "Epoch [631/1500], Loss: 0.4594, MAE: 0.4964, RMSE: 0.6929\n",
            "Epoch [632/1500], Loss: 0.4591, MAE: 0.4963, RMSE: 0.6928\n",
            "Epoch [633/1500], Loss: 0.4589, MAE: 0.4961, RMSE: 0.6926\n",
            "Epoch [634/1500], Loss: 0.4587, MAE: 0.4960, RMSE: 0.6924\n",
            "Epoch [635/1500], Loss: 0.4585, MAE: 0.4959, RMSE: 0.6923\n",
            "Epoch [636/1500], Loss: 0.4582, MAE: 0.4958, RMSE: 0.6921\n",
            "Epoch [637/1500], Loss: 0.4580, MAE: 0.4957, RMSE: 0.6919\n",
            "Epoch [638/1500], Loss: 0.4578, MAE: 0.4956, RMSE: 0.6918\n",
            "Epoch [639/1500], Loss: 0.4576, MAE: 0.4955, RMSE: 0.6916\n",
            "Epoch [640/1500], Loss: 0.4574, MAE: 0.4954, RMSE: 0.6915\n",
            "Epoch [641/1500], Loss: 0.4572, MAE: 0.4952, RMSE: 0.6913\n",
            "Epoch [642/1500], Loss: 0.4569, MAE: 0.4951, RMSE: 0.6912\n",
            "Epoch [643/1500], Loss: 0.4567, MAE: 0.4950, RMSE: 0.6910\n",
            "Epoch [644/1500], Loss: 0.4565, MAE: 0.4949, RMSE: 0.6909\n",
            "Epoch [645/1500], Loss: 0.4563, MAE: 0.4948, RMSE: 0.6907\n",
            "Epoch [646/1500], Loss: 0.4561, MAE: 0.4947, RMSE: 0.6906\n",
            "Epoch [647/1500], Loss: 0.4559, MAE: 0.4946, RMSE: 0.6904\n",
            "Epoch [648/1500], Loss: 0.4557, MAE: 0.4945, RMSE: 0.6903\n",
            "Epoch [649/1500], Loss: 0.4555, MAE: 0.4944, RMSE: 0.6901\n",
            "Epoch [650/1500], Loss: 0.4553, MAE: 0.4943, RMSE: 0.6900\n",
            "Epoch [651/1500], Loss: 0.4551, MAE: 0.4942, RMSE: 0.6898\n",
            "Epoch [652/1500], Loss: 0.4549, MAE: 0.4941, RMSE: 0.6897\n",
            "Epoch [653/1500], Loss: 0.4547, MAE: 0.4940, RMSE: 0.6896\n",
            "Epoch [654/1500], Loss: 0.4545, MAE: 0.4939, RMSE: 0.6894\n",
            "Epoch [655/1500], Loss: 0.4543, MAE: 0.4938, RMSE: 0.6893\n",
            "Epoch [656/1500], Loss: 0.4541, MAE: 0.4937, RMSE: 0.6891\n",
            "Epoch [657/1500], Loss: 0.4540, MAE: 0.4936, RMSE: 0.6890\n",
            "Epoch [658/1500], Loss: 0.4538, MAE: 0.4935, RMSE: 0.6889\n",
            "Epoch [659/1500], Loss: 0.4536, MAE: 0.4934, RMSE: 0.6887\n",
            "Epoch [660/1500], Loss: 0.4534, MAE: 0.4933, RMSE: 0.6886\n",
            "Epoch [661/1500], Loss: 0.4532, MAE: 0.4932, RMSE: 0.6885\n",
            "Epoch [662/1500], Loss: 0.4530, MAE: 0.4931, RMSE: 0.6883\n",
            "Epoch [663/1500], Loss: 0.4528, MAE: 0.4930, RMSE: 0.6882\n",
            "Epoch [664/1500], Loss: 0.4527, MAE: 0.4929, RMSE: 0.6881\n",
            "Epoch [665/1500], Loss: 0.4525, MAE: 0.4928, RMSE: 0.6879\n",
            "Epoch [666/1500], Loss: 0.4523, MAE: 0.4928, RMSE: 0.6878\n",
            "Epoch [667/1500], Loss: 0.4521, MAE: 0.4927, RMSE: 0.6877\n",
            "Epoch [668/1500], Loss: 0.4520, MAE: 0.4926, RMSE: 0.6876\n",
            "Epoch [669/1500], Loss: 0.4518, MAE: 0.4925, RMSE: 0.6874\n",
            "Epoch [670/1500], Loss: 0.4516, MAE: 0.4924, RMSE: 0.6873\n",
            "Epoch [671/1500], Loss: 0.4514, MAE: 0.4923, RMSE: 0.6872\n",
            "Epoch [672/1500], Loss: 0.4513, MAE: 0.4922, RMSE: 0.6871\n",
            "Epoch [673/1500], Loss: 0.4511, MAE: 0.4921, RMSE: 0.6869\n",
            "Epoch [674/1500], Loss: 0.4509, MAE: 0.4921, RMSE: 0.6868\n",
            "Epoch [675/1500], Loss: 0.4508, MAE: 0.4920, RMSE: 0.6867\n",
            "Epoch [676/1500], Loss: 0.4506, MAE: 0.4919, RMSE: 0.6866\n",
            "Epoch [677/1500], Loss: 0.4504, MAE: 0.4918, RMSE: 0.6864\n",
            "Epoch [678/1500], Loss: 0.4503, MAE: 0.4917, RMSE: 0.6863\n",
            "Epoch [679/1500], Loss: 0.4501, MAE: 0.4916, RMSE: 0.6862\n",
            "Epoch [680/1500], Loss: 0.4499, MAE: 0.4916, RMSE: 0.6861\n",
            "Epoch [681/1500], Loss: 0.4498, MAE: 0.4915, RMSE: 0.6860\n",
            "Epoch [682/1500], Loss: 0.4496, MAE: 0.4914, RMSE: 0.6858\n",
            "Epoch [683/1500], Loss: 0.4495, MAE: 0.4913, RMSE: 0.6857\n",
            "Epoch [684/1500], Loss: 0.4493, MAE: 0.4912, RMSE: 0.6856\n",
            "Epoch [685/1500], Loss: 0.4491, MAE: 0.4912, RMSE: 0.6855\n",
            "Epoch [686/1500], Loss: 0.4490, MAE: 0.4911, RMSE: 0.6854\n",
            "Epoch [687/1500], Loss: 0.4488, MAE: 0.4910, RMSE: 0.6852\n",
            "Epoch [688/1500], Loss: 0.4487, MAE: 0.4909, RMSE: 0.6851\n",
            "Epoch [689/1500], Loss: 0.4485, MAE: 0.4908, RMSE: 0.6850\n",
            "Epoch [690/1500], Loss: 0.4484, MAE: 0.4908, RMSE: 0.6849\n",
            "Epoch [691/1500], Loss: 0.4482, MAE: 0.4907, RMSE: 0.6848\n",
            "Epoch [692/1500], Loss: 0.4481, MAE: 0.4906, RMSE: 0.6847\n",
            "Epoch [693/1500], Loss: 0.4479, MAE: 0.4905, RMSE: 0.6845\n",
            "Epoch [694/1500], Loss: 0.4478, MAE: 0.4904, RMSE: 0.6844\n",
            "Epoch [695/1500], Loss: 0.4476, MAE: 0.4904, RMSE: 0.6843\n",
            "Epoch [696/1500], Loss: 0.4475, MAE: 0.4903, RMSE: 0.6842\n",
            "Epoch [697/1500], Loss: 0.4473, MAE: 0.4902, RMSE: 0.6841\n",
            "Epoch [698/1500], Loss: 0.4472, MAE: 0.4901, RMSE: 0.6840\n",
            "Epoch [699/1500], Loss: 0.4470, MAE: 0.4900, RMSE: 0.6838\n",
            "Epoch [700/1500], Loss: 0.4469, MAE: 0.4900, RMSE: 0.6837\n",
            "Epoch [701/1500], Loss: 0.4467, MAE: 0.4899, RMSE: 0.6836\n",
            "Epoch [702/1500], Loss: 0.4466, MAE: 0.4898, RMSE: 0.6835\n",
            "Epoch [703/1500], Loss: 0.4464, MAE: 0.4897, RMSE: 0.6834\n",
            "Epoch [704/1500], Loss: 0.4463, MAE: 0.4896, RMSE: 0.6833\n",
            "Epoch [705/1500], Loss: 0.4461, MAE: 0.4896, RMSE: 0.6831\n",
            "Epoch [706/1500], Loss: 0.4460, MAE: 0.4895, RMSE: 0.6830\n",
            "Epoch [707/1500], Loss: 0.4458, MAE: 0.4894, RMSE: 0.6829\n",
            "Epoch [708/1500], Loss: 0.4457, MAE: 0.4893, RMSE: 0.6828\n",
            "Epoch [709/1500], Loss: 0.4456, MAE: 0.4893, RMSE: 0.6827\n",
            "Epoch [710/1500], Loss: 0.4454, MAE: 0.4892, RMSE: 0.6826\n",
            "Epoch [711/1500], Loss: 0.4453, MAE: 0.4891, RMSE: 0.6825\n",
            "Epoch [712/1500], Loss: 0.4451, MAE: 0.4890, RMSE: 0.6824\n",
            "Epoch [713/1500], Loss: 0.4450, MAE: 0.4890, RMSE: 0.6823\n",
            "Epoch [714/1500], Loss: 0.4448, MAE: 0.4889, RMSE: 0.6822\n",
            "Epoch [715/1500], Loss: 0.4447, MAE: 0.4888, RMSE: 0.6820\n",
            "Epoch [716/1500], Loss: 0.4446, MAE: 0.4887, RMSE: 0.6819\n",
            "Epoch [717/1500], Loss: 0.4444, MAE: 0.4887, RMSE: 0.6818\n",
            "Epoch [718/1500], Loss: 0.4443, MAE: 0.4886, RMSE: 0.6817\n",
            "Epoch [719/1500], Loss: 0.4442, MAE: 0.4885, RMSE: 0.6816\n",
            "Epoch [720/1500], Loss: 0.4440, MAE: 0.4885, RMSE: 0.6815\n",
            "Epoch [721/1500], Loss: 0.4439, MAE: 0.4884, RMSE: 0.6814\n",
            "Epoch [722/1500], Loss: 0.4437, MAE: 0.4883, RMSE: 0.6813\n",
            "Epoch [723/1500], Loss: 0.4436, MAE: 0.4883, RMSE: 0.6812\n",
            "Epoch [724/1500], Loss: 0.4435, MAE: 0.4882, RMSE: 0.6811\n",
            "Epoch [725/1500], Loss: 0.4433, MAE: 0.4881, RMSE: 0.6810\n",
            "Epoch [726/1500], Loss: 0.4432, MAE: 0.4880, RMSE: 0.6809\n",
            "Epoch [727/1500], Loss: 0.4431, MAE: 0.4880, RMSE: 0.6808\n",
            "Epoch [728/1500], Loss: 0.4429, MAE: 0.4879, RMSE: 0.6807\n",
            "Epoch [729/1500], Loss: 0.4428, MAE: 0.4878, RMSE: 0.6806\n",
            "Epoch [730/1500], Loss: 0.4426, MAE: 0.4878, RMSE: 0.6805\n",
            "Epoch [731/1500], Loss: 0.4425, MAE: 0.4877, RMSE: 0.6804\n",
            "Epoch [732/1500], Loss: 0.4424, MAE: 0.4876, RMSE: 0.6802\n",
            "Epoch [733/1500], Loss: 0.4422, MAE: 0.4876, RMSE: 0.6801\n",
            "Epoch [734/1500], Loss: 0.4421, MAE: 0.4875, RMSE: 0.6800\n",
            "Epoch [735/1500], Loss: 0.4420, MAE: 0.4874, RMSE: 0.6799\n",
            "Epoch [736/1500], Loss: 0.4418, MAE: 0.4874, RMSE: 0.6798\n",
            "Epoch [737/1500], Loss: 0.4417, MAE: 0.4873, RMSE: 0.6797\n",
            "Epoch [738/1500], Loss: 0.4416, MAE: 0.4872, RMSE: 0.6796\n",
            "Epoch [739/1500], Loss: 0.4414, MAE: 0.4872, RMSE: 0.6795\n",
            "Epoch [740/1500], Loss: 0.4413, MAE: 0.4871, RMSE: 0.6794\n",
            "Epoch [741/1500], Loss: 0.4412, MAE: 0.4870, RMSE: 0.6793\n",
            "Epoch [742/1500], Loss: 0.4411, MAE: 0.4869, RMSE: 0.6792\n",
            "Epoch [743/1500], Loss: 0.4409, MAE: 0.4869, RMSE: 0.6791\n",
            "Epoch [744/1500], Loss: 0.4408, MAE: 0.4868, RMSE: 0.6790\n",
            "Epoch [745/1500], Loss: 0.4407, MAE: 0.4867, RMSE: 0.6789\n",
            "Epoch [746/1500], Loss: 0.4405, MAE: 0.4867, RMSE: 0.6788\n",
            "Epoch [747/1500], Loss: 0.4404, MAE: 0.4866, RMSE: 0.6787\n",
            "Epoch [748/1500], Loss: 0.4403, MAE: 0.4865, RMSE: 0.6786\n",
            "Epoch [749/1500], Loss: 0.4401, MAE: 0.4865, RMSE: 0.6785\n",
            "Epoch [750/1500], Loss: 0.4400, MAE: 0.4864, RMSE: 0.6784\n",
            "Epoch [751/1500], Loss: 0.4399, MAE: 0.4863, RMSE: 0.6783\n",
            "Epoch [752/1500], Loss: 0.4398, MAE: 0.4863, RMSE: 0.6782\n",
            "Epoch [753/1500], Loss: 0.4396, MAE: 0.4862, RMSE: 0.6781\n",
            "Epoch [754/1500], Loss: 0.4395, MAE: 0.4861, RMSE: 0.6780\n",
            "Epoch [755/1500], Loss: 0.4394, MAE: 0.4861, RMSE: 0.6779\n",
            "Epoch [756/1500], Loss: 0.4392, MAE: 0.4860, RMSE: 0.6778\n",
            "Epoch [757/1500], Loss: 0.4391, MAE: 0.4859, RMSE: 0.6777\n",
            "Epoch [758/1500], Loss: 0.4390, MAE: 0.4858, RMSE: 0.6775\n",
            "Epoch [759/1500], Loss: 0.4389, MAE: 0.4858, RMSE: 0.6774\n",
            "Epoch [760/1500], Loss: 0.4387, MAE: 0.4857, RMSE: 0.6773\n",
            "Epoch [761/1500], Loss: 0.4386, MAE: 0.4856, RMSE: 0.6772\n",
            "Epoch [762/1500], Loss: 0.4385, MAE: 0.4856, RMSE: 0.6771\n",
            "Epoch [763/1500], Loss: 0.4383, MAE: 0.4855, RMSE: 0.6770\n",
            "Epoch [764/1500], Loss: 0.4382, MAE: 0.4854, RMSE: 0.6769\n",
            "Epoch [765/1500], Loss: 0.4381, MAE: 0.4853, RMSE: 0.6768\n",
            "Epoch [766/1500], Loss: 0.4380, MAE: 0.4853, RMSE: 0.6767\n",
            "Epoch [767/1500], Loss: 0.4378, MAE: 0.4852, RMSE: 0.6766\n",
            "Epoch [768/1500], Loss: 0.4377, MAE: 0.4851, RMSE: 0.6765\n",
            "Epoch [769/1500], Loss: 0.4376, MAE: 0.4851, RMSE: 0.6764\n",
            "Epoch [770/1500], Loss: 0.4375, MAE: 0.4850, RMSE: 0.6762\n",
            "Epoch [771/1500], Loss: 0.4373, MAE: 0.4849, RMSE: 0.6761\n",
            "Epoch [772/1500], Loss: 0.4372, MAE: 0.4849, RMSE: 0.6760\n",
            "Epoch [773/1500], Loss: 0.4371, MAE: 0.4848, RMSE: 0.6759\n",
            "Epoch [774/1500], Loss: 0.4369, MAE: 0.4847, RMSE: 0.6758\n",
            "Epoch [775/1500], Loss: 0.4368, MAE: 0.4847, RMSE: 0.6757\n",
            "Epoch [776/1500], Loss: 0.4367, MAE: 0.4846, RMSE: 0.6756\n",
            "Epoch [777/1500], Loss: 0.4366, MAE: 0.4845, RMSE: 0.6755\n",
            "Epoch [778/1500], Loss: 0.4364, MAE: 0.4845, RMSE: 0.6754\n",
            "Epoch [779/1500], Loss: 0.4363, MAE: 0.4844, RMSE: 0.6753\n",
            "Epoch [780/1500], Loss: 0.4362, MAE: 0.4843, RMSE: 0.6752\n",
            "Epoch [781/1500], Loss: 0.4361, MAE: 0.4843, RMSE: 0.6751\n",
            "Epoch [782/1500], Loss: 0.4360, MAE: 0.4842, RMSE: 0.6750\n",
            "Epoch [783/1500], Loss: 0.4358, MAE: 0.4841, RMSE: 0.6749\n",
            "Epoch [784/1500], Loss: 0.4357, MAE: 0.4840, RMSE: 0.6747\n",
            "Epoch [785/1500], Loss: 0.4356, MAE: 0.4840, RMSE: 0.6746\n",
            "Epoch [786/1500], Loss: 0.4355, MAE: 0.4839, RMSE: 0.6745\n",
            "Epoch [787/1500], Loss: 0.4353, MAE: 0.4838, RMSE: 0.6744\n",
            "Epoch [788/1500], Loss: 0.4352, MAE: 0.4838, RMSE: 0.6743\n",
            "Epoch [789/1500], Loss: 0.4351, MAE: 0.4837, RMSE: 0.6742\n",
            "Epoch [790/1500], Loss: 0.4350, MAE: 0.4836, RMSE: 0.6741\n",
            "Epoch [791/1500], Loss: 0.4348, MAE: 0.4835, RMSE: 0.6740\n",
            "Epoch [792/1500], Loss: 0.4347, MAE: 0.4835, RMSE: 0.6739\n",
            "Epoch [793/1500], Loss: 0.4346, MAE: 0.4834, RMSE: 0.6738\n",
            "Epoch [794/1500], Loss: 0.4345, MAE: 0.4833, RMSE: 0.6737\n",
            "Epoch [795/1500], Loss: 0.4344, MAE: 0.4832, RMSE: 0.6736\n",
            "Epoch [796/1500], Loss: 0.4342, MAE: 0.4832, RMSE: 0.6735\n",
            "Epoch [797/1500], Loss: 0.4341, MAE: 0.4831, RMSE: 0.6734\n",
            "Epoch [798/1500], Loss: 0.4340, MAE: 0.4830, RMSE: 0.6733\n",
            "Epoch [799/1500], Loss: 0.4339, MAE: 0.4829, RMSE: 0.6731\n",
            "Epoch [800/1500], Loss: 0.4338, MAE: 0.4828, RMSE: 0.6730\n",
            "Epoch [801/1500], Loss: 0.4336, MAE: 0.4828, RMSE: 0.6729\n",
            "Epoch [802/1500], Loss: 0.4335, MAE: 0.4827, RMSE: 0.6728\n",
            "Epoch [803/1500], Loss: 0.4334, MAE: 0.4826, RMSE: 0.6727\n",
            "Epoch [804/1500], Loss: 0.4333, MAE: 0.4825, RMSE: 0.6726\n",
            "Epoch [805/1500], Loss: 0.4332, MAE: 0.4825, RMSE: 0.6725\n",
            "Epoch [806/1500], Loss: 0.4331, MAE: 0.4824, RMSE: 0.6724\n",
            "Epoch [807/1500], Loss: 0.4330, MAE: 0.4823, RMSE: 0.6723\n",
            "Epoch [808/1500], Loss: 0.4328, MAE: 0.4822, RMSE: 0.6722\n",
            "Epoch [809/1500], Loss: 0.4327, MAE: 0.4822, RMSE: 0.6721\n",
            "Epoch [810/1500], Loss: 0.4326, MAE: 0.4821, RMSE: 0.6720\n",
            "Epoch [811/1500], Loss: 0.4325, MAE: 0.4820, RMSE: 0.6719\n",
            "Epoch [812/1500], Loss: 0.4324, MAE: 0.4819, RMSE: 0.6717\n",
            "Epoch [813/1500], Loss: 0.4323, MAE: 0.4819, RMSE: 0.6716\n",
            "Epoch [814/1500], Loss: 0.4322, MAE: 0.4818, RMSE: 0.6715\n",
            "Epoch [815/1500], Loss: 0.4321, MAE: 0.4817, RMSE: 0.6714\n",
            "Epoch [816/1500], Loss: 0.4319, MAE: 0.4816, RMSE: 0.6713\n",
            "Epoch [817/1500], Loss: 0.4318, MAE: 0.4816, RMSE: 0.6712\n",
            "Epoch [818/1500], Loss: 0.4317, MAE: 0.4815, RMSE: 0.6711\n",
            "Epoch [819/1500], Loss: 0.4316, MAE: 0.4814, RMSE: 0.6710\n",
            "Epoch [820/1500], Loss: 0.4315, MAE: 0.4813, RMSE: 0.6709\n",
            "Epoch [821/1500], Loss: 0.4314, MAE: 0.4813, RMSE: 0.6708\n",
            "Epoch [822/1500], Loss: 0.4313, MAE: 0.4812, RMSE: 0.6707\n",
            "Epoch [823/1500], Loss: 0.4312, MAE: 0.4811, RMSE: 0.6706\n",
            "Epoch [824/1500], Loss: 0.4311, MAE: 0.4810, RMSE: 0.6705\n",
            "Epoch [825/1500], Loss: 0.4310, MAE: 0.4810, RMSE: 0.6704\n",
            "Epoch [826/1500], Loss: 0.4309, MAE: 0.4809, RMSE: 0.6703\n",
            "Epoch [827/1500], Loss: 0.4308, MAE: 0.4808, RMSE: 0.6702\n",
            "Epoch [828/1500], Loss: 0.4306, MAE: 0.4808, RMSE: 0.6701\n",
            "Epoch [829/1500], Loss: 0.4305, MAE: 0.4807, RMSE: 0.6700\n",
            "Epoch [830/1500], Loss: 0.4304, MAE: 0.4806, RMSE: 0.6699\n",
            "Epoch [831/1500], Loss: 0.4303, MAE: 0.4805, RMSE: 0.6698\n",
            "Epoch [832/1500], Loss: 0.4302, MAE: 0.4805, RMSE: 0.6697\n",
            "Epoch [833/1500], Loss: 0.4301, MAE: 0.4804, RMSE: 0.6696\n",
            "Epoch [834/1500], Loss: 0.4300, MAE: 0.4803, RMSE: 0.6695\n",
            "Epoch [835/1500], Loss: 0.4299, MAE: 0.4803, RMSE: 0.6694\n",
            "Epoch [836/1500], Loss: 0.4298, MAE: 0.4802, RMSE: 0.6693\n",
            "Epoch [837/1500], Loss: 0.4297, MAE: 0.4801, RMSE: 0.6692\n",
            "Epoch [838/1500], Loss: 0.4296, MAE: 0.4800, RMSE: 0.6691\n",
            "Epoch [839/1500], Loss: 0.4295, MAE: 0.4800, RMSE: 0.6690\n",
            "Epoch [840/1500], Loss: 0.4294, MAE: 0.4799, RMSE: 0.6690\n",
            "Epoch [841/1500], Loss: 0.4293, MAE: 0.4798, RMSE: 0.6689\n",
            "Epoch [842/1500], Loss: 0.4292, MAE: 0.4798, RMSE: 0.6688\n",
            "Epoch [843/1500], Loss: 0.4290, MAE: 0.4797, RMSE: 0.6687\n",
            "Epoch [844/1500], Loss: 0.4289, MAE: 0.4796, RMSE: 0.6686\n",
            "Epoch [845/1500], Loss: 0.4288, MAE: 0.4795, RMSE: 0.6685\n",
            "Epoch [846/1500], Loss: 0.4287, MAE: 0.4795, RMSE: 0.6684\n",
            "Epoch [847/1500], Loss: 0.4286, MAE: 0.4794, RMSE: 0.6683\n",
            "Epoch [848/1500], Loss: 0.4285, MAE: 0.4793, RMSE: 0.6682\n",
            "Epoch [849/1500], Loss: 0.4284, MAE: 0.4792, RMSE: 0.6681\n",
            "Epoch [850/1500], Loss: 0.4283, MAE: 0.4792, RMSE: 0.6680\n",
            "Epoch [851/1500], Loss: 0.4282, MAE: 0.4791, RMSE: 0.6679\n",
            "Epoch [852/1500], Loss: 0.4281, MAE: 0.4790, RMSE: 0.6678\n",
            "Epoch [853/1500], Loss: 0.4280, MAE: 0.4790, RMSE: 0.6678\n",
            "Epoch [854/1500], Loss: 0.4279, MAE: 0.4789, RMSE: 0.6677\n",
            "Epoch [855/1500], Loss: 0.4278, MAE: 0.4788, RMSE: 0.6676\n",
            "Epoch [856/1500], Loss: 0.4277, MAE: 0.4787, RMSE: 0.6675\n",
            "Epoch [857/1500], Loss: 0.4276, MAE: 0.4787, RMSE: 0.6674\n",
            "Epoch [858/1500], Loss: 0.4275, MAE: 0.4786, RMSE: 0.6673\n",
            "Epoch [859/1500], Loss: 0.4274, MAE: 0.4785, RMSE: 0.6672\n",
            "Epoch [860/1500], Loss: 0.4273, MAE: 0.4785, RMSE: 0.6671\n",
            "Epoch [861/1500], Loss: 0.4272, MAE: 0.4784, RMSE: 0.6670\n",
            "Epoch [862/1500], Loss: 0.4271, MAE: 0.4783, RMSE: 0.6669\n",
            "Epoch [863/1500], Loss: 0.4270, MAE: 0.4783, RMSE: 0.6668\n",
            "Epoch [864/1500], Loss: 0.4269, MAE: 0.4782, RMSE: 0.6667\n",
            "Epoch [865/1500], Loss: 0.4268, MAE: 0.4781, RMSE: 0.6666\n",
            "Epoch [866/1500], Loss: 0.4267, MAE: 0.4780, RMSE: 0.6665\n",
            "Epoch [867/1500], Loss: 0.4266, MAE: 0.4780, RMSE: 0.6664\n",
            "Epoch [868/1500], Loss: 0.4265, MAE: 0.4779, RMSE: 0.6663\n",
            "Epoch [869/1500], Loss: 0.4264, MAE: 0.4778, RMSE: 0.6662\n",
            "Epoch [870/1500], Loss: 0.4263, MAE: 0.4778, RMSE: 0.6661\n",
            "Epoch [871/1500], Loss: 0.4262, MAE: 0.4777, RMSE: 0.6660\n",
            "Epoch [872/1500], Loss: 0.4261, MAE: 0.4776, RMSE: 0.6659\n",
            "Epoch [873/1500], Loss: 0.4260, MAE: 0.4776, RMSE: 0.6659\n",
            "Epoch [874/1500], Loss: 0.4259, MAE: 0.4775, RMSE: 0.6658\n",
            "Epoch [875/1500], Loss: 0.4258, MAE: 0.4774, RMSE: 0.6657\n",
            "Epoch [876/1500], Loss: 0.4257, MAE: 0.4773, RMSE: 0.6656\n",
            "Epoch [877/1500], Loss: 0.4256, MAE: 0.4773, RMSE: 0.6655\n",
            "Epoch [878/1500], Loss: 0.4255, MAE: 0.4772, RMSE: 0.6654\n",
            "Epoch [879/1500], Loss: 0.4254, MAE: 0.4771, RMSE: 0.6653\n",
            "Epoch [880/1500], Loss: 0.4253, MAE: 0.4771, RMSE: 0.6652\n",
            "Epoch [881/1500], Loss: 0.4252, MAE: 0.4770, RMSE: 0.6652\n",
            "Epoch [882/1500], Loss: 0.4251, MAE: 0.4769, RMSE: 0.6651\n",
            "Epoch [883/1500], Loss: 0.4250, MAE: 0.4769, RMSE: 0.6650\n",
            "Epoch [884/1500], Loss: 0.4250, MAE: 0.4768, RMSE: 0.6649\n",
            "Epoch [885/1500], Loss: 0.4249, MAE: 0.4767, RMSE: 0.6648\n",
            "Epoch [886/1500], Loss: 0.4248, MAE: 0.4767, RMSE: 0.6647\n",
            "Epoch [887/1500], Loss: 0.4247, MAE: 0.4766, RMSE: 0.6646\n",
            "Epoch [888/1500], Loss: 0.4246, MAE: 0.4765, RMSE: 0.6645\n",
            "Epoch [889/1500], Loss: 0.4245, MAE: 0.4765, RMSE: 0.6644\n",
            "Epoch [890/1500], Loss: 0.4244, MAE: 0.4764, RMSE: 0.6644\n",
            "Epoch [891/1500], Loss: 0.4243, MAE: 0.4763, RMSE: 0.6643\n",
            "Epoch [892/1500], Loss: 0.4242, MAE: 0.4763, RMSE: 0.6642\n",
            "Epoch [893/1500], Loss: 0.4241, MAE: 0.4762, RMSE: 0.6641\n",
            "Epoch [894/1500], Loss: 0.4240, MAE: 0.4761, RMSE: 0.6640\n",
            "Epoch [895/1500], Loss: 0.4239, MAE: 0.4760, RMSE: 0.6639\n",
            "Epoch [896/1500], Loss: 0.4238, MAE: 0.4760, RMSE: 0.6638\n",
            "Epoch [897/1500], Loss: 0.4237, MAE: 0.4759, RMSE: 0.6638\n",
            "Epoch [898/1500], Loss: 0.4236, MAE: 0.4758, RMSE: 0.6637\n",
            "Epoch [899/1500], Loss: 0.4235, MAE: 0.4758, RMSE: 0.6636\n",
            "Epoch [900/1500], Loss: 0.4234, MAE: 0.4757, RMSE: 0.6635\n",
            "Epoch [901/1500], Loss: 0.4234, MAE: 0.4756, RMSE: 0.6634\n",
            "Epoch [902/1500], Loss: 0.4233, MAE: 0.4756, RMSE: 0.6634\n",
            "Epoch [903/1500], Loss: 0.4232, MAE: 0.4755, RMSE: 0.6633\n",
            "Epoch [904/1500], Loss: 0.4231, MAE: 0.4754, RMSE: 0.6632\n",
            "Epoch [905/1500], Loss: 0.4230, MAE: 0.4754, RMSE: 0.6631\n",
            "Epoch [906/1500], Loss: 0.4229, MAE: 0.4753, RMSE: 0.6630\n",
            "Epoch [907/1500], Loss: 0.4228, MAE: 0.4752, RMSE: 0.6629\n",
            "Epoch [908/1500], Loss: 0.4227, MAE: 0.4752, RMSE: 0.6629\n",
            "Epoch [909/1500], Loss: 0.4226, MAE: 0.4751, RMSE: 0.6628\n",
            "Epoch [910/1500], Loss: 0.4225, MAE: 0.4750, RMSE: 0.6627\n",
            "Epoch [911/1500], Loss: 0.4224, MAE: 0.4749, RMSE: 0.6626\n",
            "Epoch [912/1500], Loss: 0.4223, MAE: 0.4749, RMSE: 0.6625\n",
            "Epoch [913/1500], Loss: 0.4222, MAE: 0.4748, RMSE: 0.6624\n",
            "Epoch [914/1500], Loss: 0.4222, MAE: 0.4747, RMSE: 0.6624\n",
            "Epoch [915/1500], Loss: 0.4221, MAE: 0.4747, RMSE: 0.6623\n",
            "Epoch [916/1500], Loss: 0.4220, MAE: 0.4746, RMSE: 0.6622\n",
            "Epoch [917/1500], Loss: 0.4219, MAE: 0.4745, RMSE: 0.6621\n",
            "Epoch [918/1500], Loss: 0.4218, MAE: 0.4745, RMSE: 0.6620\n",
            "Epoch [919/1500], Loss: 0.4217, MAE: 0.4744, RMSE: 0.6620\n",
            "Epoch [920/1500], Loss: 0.4216, MAE: 0.4743, RMSE: 0.6619\n",
            "Epoch [921/1500], Loss: 0.4215, MAE: 0.4743, RMSE: 0.6618\n",
            "Epoch [922/1500], Loss: 0.4214, MAE: 0.4742, RMSE: 0.6617\n",
            "Epoch [923/1500], Loss: 0.4213, MAE: 0.4741, RMSE: 0.6616\n",
            "Epoch [924/1500], Loss: 0.4212, MAE: 0.4741, RMSE: 0.6616\n",
            "Epoch [925/1500], Loss: 0.4211, MAE: 0.4740, RMSE: 0.6615\n",
            "Epoch [926/1500], Loss: 0.4211, MAE: 0.4739, RMSE: 0.6614\n",
            "Epoch [927/1500], Loss: 0.4210, MAE: 0.4739, RMSE: 0.6613\n",
            "Epoch [928/1500], Loss: 0.4209, MAE: 0.4738, RMSE: 0.6612\n",
            "Epoch [929/1500], Loss: 0.4208, MAE: 0.4737, RMSE: 0.6612\n",
            "Epoch [930/1500], Loss: 0.4207, MAE: 0.4737, RMSE: 0.6611\n",
            "Epoch [931/1500], Loss: 0.4206, MAE: 0.4736, RMSE: 0.6610\n",
            "Epoch [932/1500], Loss: 0.4205, MAE: 0.4735, RMSE: 0.6609\n",
            "Epoch [933/1500], Loss: 0.4204, MAE: 0.4735, RMSE: 0.6608\n",
            "Epoch [934/1500], Loss: 0.4203, MAE: 0.4734, RMSE: 0.6608\n",
            "Epoch [935/1500], Loss: 0.4202, MAE: 0.4733, RMSE: 0.6607\n",
            "Epoch [936/1500], Loss: 0.4202, MAE: 0.4733, RMSE: 0.6606\n",
            "Epoch [937/1500], Loss: 0.4201, MAE: 0.4732, RMSE: 0.6605\n",
            "Epoch [938/1500], Loss: 0.4200, MAE: 0.4731, RMSE: 0.6604\n",
            "Epoch [939/1500], Loss: 0.4199, MAE: 0.4731, RMSE: 0.6604\n",
            "Epoch [940/1500], Loss: 0.4198, MAE: 0.4730, RMSE: 0.6603\n",
            "Epoch [941/1500], Loss: 0.4197, MAE: 0.4729, RMSE: 0.6602\n",
            "Epoch [942/1500], Loss: 0.4196, MAE: 0.4729, RMSE: 0.6601\n",
            "Epoch [943/1500], Loss: 0.4195, MAE: 0.4728, RMSE: 0.6601\n",
            "Epoch [944/1500], Loss: 0.4194, MAE: 0.4727, RMSE: 0.6600\n",
            "Epoch [945/1500], Loss: 0.4194, MAE: 0.4727, RMSE: 0.6599\n",
            "Epoch [946/1500], Loss: 0.4193, MAE: 0.4726, RMSE: 0.6598\n",
            "Epoch [947/1500], Loss: 0.4192, MAE: 0.4726, RMSE: 0.6598\n",
            "Epoch [948/1500], Loss: 0.4191, MAE: 0.4725, RMSE: 0.6597\n",
            "Epoch [949/1500], Loss: 0.4190, MAE: 0.4724, RMSE: 0.6596\n",
            "Epoch [950/1500], Loss: 0.4189, MAE: 0.4724, RMSE: 0.6595\n",
            "Epoch [951/1500], Loss: 0.4188, MAE: 0.4723, RMSE: 0.6595\n",
            "Epoch [952/1500], Loss: 0.4187, MAE: 0.4722, RMSE: 0.6594\n",
            "Epoch [953/1500], Loss: 0.4186, MAE: 0.4722, RMSE: 0.6593\n",
            "Epoch [954/1500], Loss: 0.4186, MAE: 0.4721, RMSE: 0.6592\n",
            "Epoch [955/1500], Loss: 0.4185, MAE: 0.4721, RMSE: 0.6592\n",
            "Epoch [956/1500], Loss: 0.4184, MAE: 0.4720, RMSE: 0.6591\n",
            "Epoch [957/1500], Loss: 0.4183, MAE: 0.4719, RMSE: 0.6590\n",
            "Epoch [958/1500], Loss: 0.4182, MAE: 0.4719, RMSE: 0.6589\n",
            "Epoch [959/1500], Loss: 0.4181, MAE: 0.4718, RMSE: 0.6588\n",
            "Epoch [960/1500], Loss: 0.4180, MAE: 0.4717, RMSE: 0.6588\n",
            "Epoch [961/1500], Loss: 0.4179, MAE: 0.4717, RMSE: 0.6587\n",
            "Epoch [962/1500], Loss: 0.4179, MAE: 0.4716, RMSE: 0.6586\n",
            "Epoch [963/1500], Loss: 0.4178, MAE: 0.4715, RMSE: 0.6585\n",
            "Epoch [964/1500], Loss: 0.4177, MAE: 0.4715, RMSE: 0.6585\n",
            "Epoch [965/1500], Loss: 0.4176, MAE: 0.4714, RMSE: 0.6584\n",
            "Epoch [966/1500], Loss: 0.4175, MAE: 0.4714, RMSE: 0.6583\n",
            "Epoch [967/1500], Loss: 0.4174, MAE: 0.4713, RMSE: 0.6582\n",
            "Epoch [968/1500], Loss: 0.4173, MAE: 0.4712, RMSE: 0.6581\n",
            "Epoch [969/1500], Loss: 0.4172, MAE: 0.4712, RMSE: 0.6581\n",
            "Epoch [970/1500], Loss: 0.4172, MAE: 0.4711, RMSE: 0.6580\n",
            "Epoch [971/1500], Loss: 0.4171, MAE: 0.4711, RMSE: 0.6579\n",
            "Epoch [972/1500], Loss: 0.4170, MAE: 0.4710, RMSE: 0.6578\n",
            "Epoch [973/1500], Loss: 0.4169, MAE: 0.4709, RMSE: 0.6578\n",
            "Epoch [974/1500], Loss: 0.4168, MAE: 0.4709, RMSE: 0.6577\n",
            "Epoch [975/1500], Loss: 0.4167, MAE: 0.4708, RMSE: 0.6576\n",
            "Epoch [976/1500], Loss: 0.4166, MAE: 0.4707, RMSE: 0.6575\n",
            "Epoch [977/1500], Loss: 0.4166, MAE: 0.4707, RMSE: 0.6574\n",
            "Epoch [978/1500], Loss: 0.4165, MAE: 0.4706, RMSE: 0.6574\n",
            "Epoch [979/1500], Loss: 0.4164, MAE: 0.4706, RMSE: 0.6573\n",
            "Epoch [980/1500], Loss: 0.4163, MAE: 0.4705, RMSE: 0.6572\n",
            "Epoch [981/1500], Loss: 0.4162, MAE: 0.4704, RMSE: 0.6571\n",
            "Epoch [982/1500], Loss: 0.4161, MAE: 0.4704, RMSE: 0.6570\n",
            "Epoch [983/1500], Loss: 0.4160, MAE: 0.4703, RMSE: 0.6570\n",
            "Epoch [984/1500], Loss: 0.4160, MAE: 0.4702, RMSE: 0.6569\n",
            "Epoch [985/1500], Loss: 0.4159, MAE: 0.4702, RMSE: 0.6568\n",
            "Epoch [986/1500], Loss: 0.4158, MAE: 0.4701, RMSE: 0.6567\n",
            "Epoch [987/1500], Loss: 0.4157, MAE: 0.4701, RMSE: 0.6567\n",
            "Epoch [988/1500], Loss: 0.4156, MAE: 0.4700, RMSE: 0.6566\n",
            "Epoch [989/1500], Loss: 0.4155, MAE: 0.4699, RMSE: 0.6565\n",
            "Epoch [990/1500], Loss: 0.4155, MAE: 0.4699, RMSE: 0.6564\n",
            "Epoch [991/1500], Loss: 0.4154, MAE: 0.4698, RMSE: 0.6563\n",
            "Epoch [992/1500], Loss: 0.4153, MAE: 0.4698, RMSE: 0.6563\n",
            "Epoch [993/1500], Loss: 0.4152, MAE: 0.4697, RMSE: 0.6562\n",
            "Epoch [994/1500], Loss: 0.4151, MAE: 0.4696, RMSE: 0.6561\n",
            "Epoch [995/1500], Loss: 0.4150, MAE: 0.4696, RMSE: 0.6560\n",
            "Epoch [996/1500], Loss: 0.4149, MAE: 0.4695, RMSE: 0.6560\n",
            "Epoch [997/1500], Loss: 0.4149, MAE: 0.4694, RMSE: 0.6559\n",
            "Epoch [998/1500], Loss: 0.4148, MAE: 0.4694, RMSE: 0.6558\n",
            "Epoch [999/1500], Loss: 0.4147, MAE: 0.4693, RMSE: 0.6557\n",
            "Epoch [1000/1500], Loss: 0.4146, MAE: 0.4692, RMSE: 0.6556\n",
            "Epoch [1001/1500], Loss: 0.4145, MAE: 0.4692, RMSE: 0.6556\n",
            "Epoch [1002/1500], Loss: 0.4144, MAE: 0.4691, RMSE: 0.6555\n",
            "Epoch [1003/1500], Loss: 0.4143, MAE: 0.4691, RMSE: 0.6554\n",
            "Epoch [1004/1500], Loss: 0.4142, MAE: 0.4690, RMSE: 0.6553\n",
            "Epoch [1005/1500], Loss: 0.4142, MAE: 0.4689, RMSE: 0.6553\n",
            "Epoch [1006/1500], Loss: 0.4141, MAE: 0.4689, RMSE: 0.6552\n",
            "Epoch [1007/1500], Loss: 0.4140, MAE: 0.4688, RMSE: 0.6551\n",
            "Epoch [1008/1500], Loss: 0.4139, MAE: 0.4688, RMSE: 0.6550\n",
            "Epoch [1009/1500], Loss: 0.4138, MAE: 0.4687, RMSE: 0.6549\n",
            "Epoch [1010/1500], Loss: 0.4137, MAE: 0.4686, RMSE: 0.6549\n",
            "Epoch [1011/1500], Loss: 0.4136, MAE: 0.4686, RMSE: 0.6548\n",
            "Epoch [1012/1500], Loss: 0.4136, MAE: 0.4685, RMSE: 0.6547\n",
            "Epoch [1013/1500], Loss: 0.4135, MAE: 0.4685, RMSE: 0.6546\n",
            "Epoch [1014/1500], Loss: 0.4134, MAE: 0.4684, RMSE: 0.6546\n",
            "Epoch [1015/1500], Loss: 0.4133, MAE: 0.4683, RMSE: 0.6545\n",
            "Epoch [1016/1500], Loss: 0.4132, MAE: 0.4683, RMSE: 0.6544\n",
            "Epoch [1017/1500], Loss: 0.4131, MAE: 0.4682, RMSE: 0.6543\n",
            "Epoch [1018/1500], Loss: 0.4130, MAE: 0.4682, RMSE: 0.6542\n",
            "Epoch [1019/1500], Loss: 0.4129, MAE: 0.4681, RMSE: 0.6542\n",
            "Epoch [1020/1500], Loss: 0.4129, MAE: 0.4680, RMSE: 0.6541\n",
            "Epoch [1021/1500], Loss: 0.4128, MAE: 0.4680, RMSE: 0.6540\n",
            "Epoch [1022/1500], Loss: 0.4127, MAE: 0.4679, RMSE: 0.6539\n",
            "Epoch [1023/1500], Loss: 0.4126, MAE: 0.4679, RMSE: 0.6538\n",
            "Epoch [1024/1500], Loss: 0.4125, MAE: 0.4678, RMSE: 0.6538\n",
            "Epoch [1025/1500], Loss: 0.4124, MAE: 0.4677, RMSE: 0.6537\n",
            "Epoch [1026/1500], Loss: 0.4123, MAE: 0.4677, RMSE: 0.6536\n",
            "Epoch [1027/1500], Loss: 0.4122, MAE: 0.4676, RMSE: 0.6535\n",
            "Epoch [1028/1500], Loss: 0.4122, MAE: 0.4675, RMSE: 0.6535\n",
            "Epoch [1029/1500], Loss: 0.4121, MAE: 0.4675, RMSE: 0.6534\n",
            "Epoch [1030/1500], Loss: 0.4120, MAE: 0.4674, RMSE: 0.6533\n",
            "Epoch [1031/1500], Loss: 0.4119, MAE: 0.4674, RMSE: 0.6532\n",
            "Epoch [1032/1500], Loss: 0.4118, MAE: 0.4673, RMSE: 0.6531\n",
            "Epoch [1033/1500], Loss: 0.4117, MAE: 0.4672, RMSE: 0.6531\n",
            "Epoch [1034/1500], Loss: 0.4116, MAE: 0.4672, RMSE: 0.6530\n",
            "Epoch [1035/1500], Loss: 0.4115, MAE: 0.4671, RMSE: 0.6529\n",
            "Epoch [1036/1500], Loss: 0.4115, MAE: 0.4671, RMSE: 0.6528\n",
            "Epoch [1037/1500], Loss: 0.4114, MAE: 0.4670, RMSE: 0.6528\n",
            "Epoch [1038/1500], Loss: 0.4113, MAE: 0.4669, RMSE: 0.6527\n",
            "Epoch [1039/1500], Loss: 0.4112, MAE: 0.4669, RMSE: 0.6526\n",
            "Epoch [1040/1500], Loss: 0.4111, MAE: 0.4668, RMSE: 0.6525\n",
            "Epoch [1041/1500], Loss: 0.4110, MAE: 0.4668, RMSE: 0.6524\n",
            "Epoch [1042/1500], Loss: 0.4109, MAE: 0.4667, RMSE: 0.6524\n",
            "Epoch [1043/1500], Loss: 0.4109, MAE: 0.4666, RMSE: 0.6523\n",
            "Epoch [1044/1500], Loss: 0.4108, MAE: 0.4666, RMSE: 0.6522\n",
            "Epoch [1045/1500], Loss: 0.4107, MAE: 0.4665, RMSE: 0.6521\n",
            "Epoch [1046/1500], Loss: 0.4106, MAE: 0.4665, RMSE: 0.6521\n",
            "Epoch [1047/1500], Loss: 0.4105, MAE: 0.4664, RMSE: 0.6520\n",
            "Epoch [1048/1500], Loss: 0.4104, MAE: 0.4663, RMSE: 0.6519\n",
            "Epoch [1049/1500], Loss: 0.4103, MAE: 0.4663, RMSE: 0.6518\n",
            "Epoch [1050/1500], Loss: 0.4103, MAE: 0.4662, RMSE: 0.6518\n",
            "Epoch [1051/1500], Loss: 0.4102, MAE: 0.4662, RMSE: 0.6517\n",
            "Epoch [1052/1500], Loss: 0.4101, MAE: 0.4661, RMSE: 0.6516\n",
            "Epoch [1053/1500], Loss: 0.4100, MAE: 0.4660, RMSE: 0.6515\n",
            "Epoch [1054/1500], Loss: 0.4099, MAE: 0.4660, RMSE: 0.6515\n",
            "Epoch [1055/1500], Loss: 0.4098, MAE: 0.4659, RMSE: 0.6514\n",
            "Epoch [1056/1500], Loss: 0.4098, MAE: 0.4659, RMSE: 0.6513\n",
            "Epoch [1057/1500], Loss: 0.4097, MAE: 0.4658, RMSE: 0.6512\n",
            "Epoch [1058/1500], Loss: 0.4096, MAE: 0.4657, RMSE: 0.6511\n",
            "Epoch [1059/1500], Loss: 0.4095, MAE: 0.4657, RMSE: 0.6511\n",
            "Epoch [1060/1500], Loss: 0.4094, MAE: 0.4656, RMSE: 0.6510\n",
            "Epoch [1061/1500], Loss: 0.4094, MAE: 0.4656, RMSE: 0.6509\n",
            "Epoch [1062/1500], Loss: 0.4093, MAE: 0.4655, RMSE: 0.6508\n",
            "Epoch [1063/1500], Loss: 0.4092, MAE: 0.4654, RMSE: 0.6507\n",
            "Epoch [1064/1500], Loss: 0.4091, MAE: 0.4654, RMSE: 0.6507\n",
            "Epoch [1065/1500], Loss: 0.4090, MAE: 0.4653, RMSE: 0.6506\n",
            "Epoch [1066/1500], Loss: 0.4089, MAE: 0.4653, RMSE: 0.6505\n",
            "Epoch [1067/1500], Loss: 0.4089, MAE: 0.4652, RMSE: 0.6504\n",
            "Epoch [1068/1500], Loss: 0.4088, MAE: 0.4651, RMSE: 0.6504\n",
            "Epoch [1069/1500], Loss: 0.4087, MAE: 0.4651, RMSE: 0.6503\n",
            "Epoch [1070/1500], Loss: 0.4086, MAE: 0.4650, RMSE: 0.6502\n",
            "Epoch [1071/1500], Loss: 0.4086, MAE: 0.4650, RMSE: 0.6501\n",
            "Epoch [1072/1500], Loss: 0.4085, MAE: 0.4649, RMSE: 0.6501\n",
            "Epoch [1073/1500], Loss: 0.4084, MAE: 0.4649, RMSE: 0.6500\n",
            "Epoch [1074/1500], Loss: 0.4083, MAE: 0.4648, RMSE: 0.6499\n",
            "Epoch [1075/1500], Loss: 0.4082, MAE: 0.4647, RMSE: 0.6498\n",
            "Epoch [1076/1500], Loss: 0.4082, MAE: 0.4647, RMSE: 0.6498\n",
            "Epoch [1077/1500], Loss: 0.4081, MAE: 0.4646, RMSE: 0.6497\n",
            "Epoch [1078/1500], Loss: 0.4080, MAE: 0.4646, RMSE: 0.6496\n",
            "Epoch [1079/1500], Loss: 0.4079, MAE: 0.4645, RMSE: 0.6496\n",
            "Epoch [1080/1500], Loss: 0.4078, MAE: 0.4644, RMSE: 0.6495\n",
            "Epoch [1081/1500], Loss: 0.4078, MAE: 0.4644, RMSE: 0.6494\n",
            "Epoch [1082/1500], Loss: 0.4077, MAE: 0.4643, RMSE: 0.6493\n",
            "Epoch [1083/1500], Loss: 0.4076, MAE: 0.4643, RMSE: 0.6493\n",
            "Epoch [1084/1500], Loss: 0.4075, MAE: 0.4642, RMSE: 0.6492\n",
            "Epoch [1085/1500], Loss: 0.4075, MAE: 0.4641, RMSE: 0.6491\n",
            "Epoch [1086/1500], Loss: 0.4074, MAE: 0.4641, RMSE: 0.6491\n",
            "Epoch [1087/1500], Loss: 0.4073, MAE: 0.4640, RMSE: 0.6490\n",
            "Epoch [1088/1500], Loss: 0.4072, MAE: 0.4640, RMSE: 0.6489\n",
            "Epoch [1089/1500], Loss: 0.4072, MAE: 0.4639, RMSE: 0.6489\n",
            "Epoch [1090/1500], Loss: 0.4071, MAE: 0.4638, RMSE: 0.6488\n",
            "Epoch [1091/1500], Loss: 0.4070, MAE: 0.4638, RMSE: 0.6487\n",
            "Epoch [1092/1500], Loss: 0.4069, MAE: 0.4637, RMSE: 0.6486\n",
            "Epoch [1093/1500], Loss: 0.4069, MAE: 0.4637, RMSE: 0.6486\n",
            "Epoch [1094/1500], Loss: 0.4068, MAE: 0.4636, RMSE: 0.6485\n",
            "Epoch [1095/1500], Loss: 0.4067, MAE: 0.4635, RMSE: 0.6484\n",
            "Epoch [1096/1500], Loss: 0.4066, MAE: 0.4635, RMSE: 0.6484\n",
            "Epoch [1097/1500], Loss: 0.4065, MAE: 0.4634, RMSE: 0.6483\n",
            "Epoch [1098/1500], Loss: 0.4065, MAE: 0.4634, RMSE: 0.6482\n",
            "Epoch [1099/1500], Loss: 0.4064, MAE: 0.4633, RMSE: 0.6482\n",
            "Epoch [1100/1500], Loss: 0.4063, MAE: 0.4633, RMSE: 0.6481\n",
            "Epoch [1101/1500], Loss: 0.4062, MAE: 0.4632, RMSE: 0.6480\n",
            "Epoch [1102/1500], Loss: 0.4062, MAE: 0.4632, RMSE: 0.6480\n",
            "Epoch [1103/1500], Loss: 0.4061, MAE: 0.4631, RMSE: 0.6479\n",
            "Epoch [1104/1500], Loss: 0.4060, MAE: 0.4630, RMSE: 0.6478\n",
            "Epoch [1105/1500], Loss: 0.4059, MAE: 0.4630, RMSE: 0.6478\n",
            "Epoch [1106/1500], Loss: 0.4059, MAE: 0.4629, RMSE: 0.6477\n",
            "Epoch [1107/1500], Loss: 0.4058, MAE: 0.4629, RMSE: 0.6476\n",
            "Epoch [1108/1500], Loss: 0.4057, MAE: 0.4628, RMSE: 0.6476\n",
            "Epoch [1109/1500], Loss: 0.4056, MAE: 0.4628, RMSE: 0.6475\n",
            "Epoch [1110/1500], Loss: 0.4056, MAE: 0.4627, RMSE: 0.6474\n",
            "Epoch [1111/1500], Loss: 0.4055, MAE: 0.4627, RMSE: 0.6474\n",
            "Epoch [1112/1500], Loss: 0.4054, MAE: 0.4626, RMSE: 0.6473\n",
            "Epoch [1113/1500], Loss: 0.4053, MAE: 0.4626, RMSE: 0.6472\n",
            "Epoch [1114/1500], Loss: 0.4053, MAE: 0.4625, RMSE: 0.6472\n",
            "Epoch [1115/1500], Loss: 0.4052, MAE: 0.4625, RMSE: 0.6471\n",
            "Epoch [1116/1500], Loss: 0.4051, MAE: 0.4624, RMSE: 0.6470\n",
            "Epoch [1117/1500], Loss: 0.4050, MAE: 0.4624, RMSE: 0.6470\n",
            "Epoch [1118/1500], Loss: 0.4050, MAE: 0.4623, RMSE: 0.6469\n",
            "Epoch [1119/1500], Loss: 0.4049, MAE: 0.4623, RMSE: 0.6468\n",
            "Epoch [1120/1500], Loss: 0.4048, MAE: 0.4622, RMSE: 0.6468\n",
            "Epoch [1121/1500], Loss: 0.4047, MAE: 0.4622, RMSE: 0.6467\n",
            "Epoch [1122/1500], Loss: 0.4047, MAE: 0.4621, RMSE: 0.6466\n",
            "Epoch [1123/1500], Loss: 0.4046, MAE: 0.4620, RMSE: 0.6466\n",
            "Epoch [1124/1500], Loss: 0.4045, MAE: 0.4620, RMSE: 0.6465\n",
            "Epoch [1125/1500], Loss: 0.4044, MAE: 0.4619, RMSE: 0.6465\n",
            "Epoch [1126/1500], Loss: 0.4044, MAE: 0.4619, RMSE: 0.6464\n",
            "Epoch [1127/1500], Loss: 0.4043, MAE: 0.4618, RMSE: 0.6463\n",
            "Epoch [1128/1500], Loss: 0.4042, MAE: 0.4618, RMSE: 0.6463\n",
            "Epoch [1129/1500], Loss: 0.4041, MAE: 0.4617, RMSE: 0.6462\n",
            "Epoch [1130/1500], Loss: 0.4041, MAE: 0.4617, RMSE: 0.6461\n",
            "Epoch [1131/1500], Loss: 0.4040, MAE: 0.4616, RMSE: 0.6461\n",
            "Epoch [1132/1500], Loss: 0.4039, MAE: 0.4616, RMSE: 0.6460\n",
            "Epoch [1133/1500], Loss: 0.4038, MAE: 0.4615, RMSE: 0.6459\n",
            "Epoch [1134/1500], Loss: 0.4038, MAE: 0.4615, RMSE: 0.6459\n",
            "Epoch [1135/1500], Loss: 0.4037, MAE: 0.4614, RMSE: 0.6458\n",
            "Epoch [1136/1500], Loss: 0.4036, MAE: 0.4614, RMSE: 0.6457\n",
            "Epoch [1137/1500], Loss: 0.4035, MAE: 0.4613, RMSE: 0.6457\n",
            "Epoch [1138/1500], Loss: 0.4035, MAE: 0.4613, RMSE: 0.6456\n",
            "Epoch [1139/1500], Loss: 0.4034, MAE: 0.4612, RMSE: 0.6455\n",
            "Epoch [1140/1500], Loss: 0.4033, MAE: 0.4612, RMSE: 0.6455\n",
            "Epoch [1141/1500], Loss: 0.4032, MAE: 0.4611, RMSE: 0.6454\n",
            "Epoch [1142/1500], Loss: 0.4032, MAE: 0.4611, RMSE: 0.6453\n",
            "Epoch [1143/1500], Loss: 0.4031, MAE: 0.4610, RMSE: 0.6453\n",
            "Epoch [1144/1500], Loss: 0.4030, MAE: 0.4610, RMSE: 0.6452\n",
            "Epoch [1145/1500], Loss: 0.4029, MAE: 0.4609, RMSE: 0.6451\n",
            "Epoch [1146/1500], Loss: 0.4029, MAE: 0.4609, RMSE: 0.6451\n",
            "Epoch [1147/1500], Loss: 0.4028, MAE: 0.4608, RMSE: 0.6450\n",
            "Epoch [1148/1500], Loss: 0.4027, MAE: 0.4608, RMSE: 0.6450\n",
            "Epoch [1149/1500], Loss: 0.4026, MAE: 0.4607, RMSE: 0.6449\n",
            "Epoch [1150/1500], Loss: 0.4026, MAE: 0.4606, RMSE: 0.6448\n",
            "Epoch [1151/1500], Loss: 0.4025, MAE: 0.4606, RMSE: 0.6448\n",
            "Epoch [1152/1500], Loss: 0.4024, MAE: 0.4605, RMSE: 0.6447\n",
            "Epoch [1153/1500], Loss: 0.4024, MAE: 0.4605, RMSE: 0.6446\n",
            "Epoch [1154/1500], Loss: 0.4023, MAE: 0.4604, RMSE: 0.6446\n",
            "Epoch [1155/1500], Loss: 0.4022, MAE: 0.4604, RMSE: 0.6445\n",
            "Epoch [1156/1500], Loss: 0.4021, MAE: 0.4603, RMSE: 0.6444\n",
            "Epoch [1157/1500], Loss: 0.4021, MAE: 0.4603, RMSE: 0.6444\n",
            "Epoch [1158/1500], Loss: 0.4020, MAE: 0.4602, RMSE: 0.6443\n",
            "Epoch [1159/1500], Loss: 0.4019, MAE: 0.4601, RMSE: 0.6443\n",
            "Epoch [1160/1500], Loss: 0.4019, MAE: 0.4601, RMSE: 0.6442\n",
            "Epoch [1161/1500], Loss: 0.4018, MAE: 0.4600, RMSE: 0.6441\n",
            "Epoch [1162/1500], Loss: 0.4017, MAE: 0.4600, RMSE: 0.6441\n",
            "Epoch [1163/1500], Loss: 0.4016, MAE: 0.4599, RMSE: 0.6440\n",
            "Epoch [1164/1500], Loss: 0.4016, MAE: 0.4599, RMSE: 0.6439\n",
            "Epoch [1165/1500], Loss: 0.4015, MAE: 0.4598, RMSE: 0.6439\n",
            "Epoch [1166/1500], Loss: 0.4014, MAE: 0.4598, RMSE: 0.6438\n",
            "Epoch [1167/1500], Loss: 0.4014, MAE: 0.4597, RMSE: 0.6437\n",
            "Epoch [1168/1500], Loss: 0.4013, MAE: 0.4597, RMSE: 0.6437\n",
            "Epoch [1169/1500], Loss: 0.4012, MAE: 0.4596, RMSE: 0.6436\n",
            "Epoch [1170/1500], Loss: 0.4011, MAE: 0.4596, RMSE: 0.6435\n",
            "Epoch [1171/1500], Loss: 0.4011, MAE: 0.4595, RMSE: 0.6435\n",
            "Epoch [1172/1500], Loss: 0.4010, MAE: 0.4595, RMSE: 0.6434\n",
            "Epoch [1173/1500], Loss: 0.4009, MAE: 0.4594, RMSE: 0.6433\n",
            "Epoch [1174/1500], Loss: 0.4009, MAE: 0.4594, RMSE: 0.6433\n",
            "Epoch [1175/1500], Loss: 0.4008, MAE: 0.4593, RMSE: 0.6432\n",
            "Epoch [1176/1500], Loss: 0.4007, MAE: 0.4593, RMSE: 0.6431\n",
            "Epoch [1177/1500], Loss: 0.4007, MAE: 0.4592, RMSE: 0.6431\n",
            "Epoch [1178/1500], Loss: 0.4006, MAE: 0.4591, RMSE: 0.6430\n",
            "Epoch [1179/1500], Loss: 0.4005, MAE: 0.4591, RMSE: 0.6429\n",
            "Epoch [1180/1500], Loss: 0.4005, MAE: 0.4590, RMSE: 0.6429\n",
            "Epoch [1181/1500], Loss: 0.4004, MAE: 0.4590, RMSE: 0.6428\n",
            "Epoch [1182/1500], Loss: 0.4003, MAE: 0.4589, RMSE: 0.6428\n",
            "Epoch [1183/1500], Loss: 0.4003, MAE: 0.4589, RMSE: 0.6427\n",
            "Epoch [1184/1500], Loss: 0.4002, MAE: 0.4588, RMSE: 0.6426\n",
            "Epoch [1185/1500], Loss: 0.4001, MAE: 0.4588, RMSE: 0.6426\n",
            "Epoch [1186/1500], Loss: 0.4001, MAE: 0.4587, RMSE: 0.6425\n",
            "Epoch [1187/1500], Loss: 0.4000, MAE: 0.4587, RMSE: 0.6424\n",
            "Epoch [1188/1500], Loss: 0.3999, MAE: 0.4586, RMSE: 0.6424\n",
            "Epoch [1189/1500], Loss: 0.3999, MAE: 0.4586, RMSE: 0.6423\n",
            "Epoch [1190/1500], Loss: 0.3998, MAE: 0.4585, RMSE: 0.6422\n",
            "Epoch [1191/1500], Loss: 0.3997, MAE: 0.4585, RMSE: 0.6422\n",
            "Epoch [1192/1500], Loss: 0.3996, MAE: 0.4584, RMSE: 0.6421\n",
            "Epoch [1193/1500], Loss: 0.3996, MAE: 0.4584, RMSE: 0.6420\n",
            "Epoch [1194/1500], Loss: 0.3995, MAE: 0.4583, RMSE: 0.6420\n",
            "Epoch [1195/1500], Loss: 0.3994, MAE: 0.4583, RMSE: 0.6419\n",
            "Epoch [1196/1500], Loss: 0.3994, MAE: 0.4582, RMSE: 0.6419\n",
            "Epoch [1197/1500], Loss: 0.3993, MAE: 0.4582, RMSE: 0.6418\n",
            "Epoch [1198/1500], Loss: 0.3992, MAE: 0.4581, RMSE: 0.6417\n",
            "Epoch [1199/1500], Loss: 0.3992, MAE: 0.4581, RMSE: 0.6417\n",
            "Epoch [1200/1500], Loss: 0.3991, MAE: 0.4580, RMSE: 0.6416\n",
            "Epoch [1201/1500], Loss: 0.3990, MAE: 0.4580, RMSE: 0.6415\n",
            "Epoch [1202/1500], Loss: 0.3990, MAE: 0.4579, RMSE: 0.6415\n",
            "Epoch [1203/1500], Loss: 0.3989, MAE: 0.4579, RMSE: 0.6414\n",
            "Epoch [1204/1500], Loss: 0.3988, MAE: 0.4578, RMSE: 0.6413\n",
            "Epoch [1205/1500], Loss: 0.3988, MAE: 0.4578, RMSE: 0.6413\n",
            "Epoch [1206/1500], Loss: 0.3987, MAE: 0.4577, RMSE: 0.6412\n",
            "Epoch [1207/1500], Loss: 0.3986, MAE: 0.4577, RMSE: 0.6412\n",
            "Epoch [1208/1500], Loss: 0.3986, MAE: 0.4576, RMSE: 0.6411\n",
            "Epoch [1209/1500], Loss: 0.3985, MAE: 0.4576, RMSE: 0.6410\n",
            "Epoch [1210/1500], Loss: 0.3984, MAE: 0.4575, RMSE: 0.6410\n",
            "Epoch [1211/1500], Loss: 0.3984, MAE: 0.4575, RMSE: 0.6409\n",
            "Epoch [1212/1500], Loss: 0.3983, MAE: 0.4574, RMSE: 0.6408\n",
            "Epoch [1213/1500], Loss: 0.3982, MAE: 0.4574, RMSE: 0.6408\n",
            "Epoch [1214/1500], Loss: 0.3982, MAE: 0.4573, RMSE: 0.6407\n",
            "Epoch [1215/1500], Loss: 0.3981, MAE: 0.4573, RMSE: 0.6407\n",
            "Epoch [1216/1500], Loss: 0.3980, MAE: 0.4572, RMSE: 0.6406\n",
            "Epoch [1217/1500], Loss: 0.3980, MAE: 0.4572, RMSE: 0.6405\n",
            "Epoch [1218/1500], Loss: 0.3979, MAE: 0.4571, RMSE: 0.6405\n",
            "Epoch [1219/1500], Loss: 0.3978, MAE: 0.4571, RMSE: 0.6404\n",
            "Epoch [1220/1500], Loss: 0.3978, MAE: 0.4570, RMSE: 0.6403\n",
            "Epoch [1221/1500], Loss: 0.3977, MAE: 0.4570, RMSE: 0.6403\n",
            "Epoch [1222/1500], Loss: 0.3977, MAE: 0.4569, RMSE: 0.6402\n",
            "Epoch [1223/1500], Loss: 0.3976, MAE: 0.4569, RMSE: 0.6402\n",
            "Epoch [1224/1500], Loss: 0.3975, MAE: 0.4568, RMSE: 0.6401\n",
            "Epoch [1225/1500], Loss: 0.3975, MAE: 0.4568, RMSE: 0.6401\n",
            "Epoch [1226/1500], Loss: 0.3974, MAE: 0.4568, RMSE: 0.6400\n",
            "Epoch [1227/1500], Loss: 0.3973, MAE: 0.4567, RMSE: 0.6399\n",
            "Epoch [1228/1500], Loss: 0.3973, MAE: 0.4567, RMSE: 0.6399\n",
            "Epoch [1229/1500], Loss: 0.3972, MAE: 0.4566, RMSE: 0.6398\n",
            "Epoch [1230/1500], Loss: 0.3971, MAE: 0.4566, RMSE: 0.6398\n",
            "Epoch [1231/1500], Loss: 0.3971, MAE: 0.4565, RMSE: 0.6397\n",
            "Epoch [1232/1500], Loss: 0.3970, MAE: 0.4565, RMSE: 0.6397\n",
            "Epoch [1233/1500], Loss: 0.3970, MAE: 0.4564, RMSE: 0.6396\n",
            "Epoch [1234/1500], Loss: 0.3969, MAE: 0.4564, RMSE: 0.6395\n",
            "Epoch [1235/1500], Loss: 0.3968, MAE: 0.4564, RMSE: 0.6395\n",
            "Epoch [1236/1500], Loss: 0.3968, MAE: 0.4563, RMSE: 0.6394\n",
            "Epoch [1237/1500], Loss: 0.3967, MAE: 0.4563, RMSE: 0.6394\n",
            "Epoch [1238/1500], Loss: 0.3967, MAE: 0.4562, RMSE: 0.6393\n",
            "Epoch [1239/1500], Loss: 0.3966, MAE: 0.4562, RMSE: 0.6393\n",
            "Epoch [1240/1500], Loss: 0.3965, MAE: 0.4561, RMSE: 0.6392\n",
            "Epoch [1241/1500], Loss: 0.3965, MAE: 0.4561, RMSE: 0.6392\n",
            "Epoch [1242/1500], Loss: 0.3964, MAE: 0.4561, RMSE: 0.6391\n",
            "Epoch [1243/1500], Loss: 0.3964, MAE: 0.4560, RMSE: 0.6390\n",
            "Epoch [1244/1500], Loss: 0.3963, MAE: 0.4560, RMSE: 0.6390\n",
            "Epoch [1245/1500], Loss: 0.3962, MAE: 0.4559, RMSE: 0.6389\n",
            "Epoch [1246/1500], Loss: 0.3962, MAE: 0.4559, RMSE: 0.6389\n",
            "Epoch [1247/1500], Loss: 0.3961, MAE: 0.4558, RMSE: 0.6388\n",
            "Epoch [1248/1500], Loss: 0.3961, MAE: 0.4558, RMSE: 0.6388\n",
            "Epoch [1249/1500], Loss: 0.3960, MAE: 0.4557, RMSE: 0.6387\n",
            "Epoch [1250/1500], Loss: 0.3959, MAE: 0.4557, RMSE: 0.6386\n",
            "Epoch [1251/1500], Loss: 0.3959, MAE: 0.4556, RMSE: 0.6386\n",
            "Epoch [1252/1500], Loss: 0.3958, MAE: 0.4556, RMSE: 0.6385\n",
            "Epoch [1253/1500], Loss: 0.3958, MAE: 0.4556, RMSE: 0.6385\n",
            "Epoch [1254/1500], Loss: 0.3957, MAE: 0.4555, RMSE: 0.6384\n",
            "Epoch [1255/1500], Loss: 0.3956, MAE: 0.4555, RMSE: 0.6384\n",
            "Epoch [1256/1500], Loss: 0.3956, MAE: 0.4554, RMSE: 0.6383\n",
            "Epoch [1257/1500], Loss: 0.3955, MAE: 0.4554, RMSE: 0.6383\n",
            "Epoch [1258/1500], Loss: 0.3955, MAE: 0.4553, RMSE: 0.6382\n",
            "Epoch [1259/1500], Loss: 0.3954, MAE: 0.4553, RMSE: 0.6381\n",
            "Epoch [1260/1500], Loss: 0.3953, MAE: 0.4552, RMSE: 0.6381\n",
            "Epoch [1261/1500], Loss: 0.3953, MAE: 0.4552, RMSE: 0.6380\n",
            "Epoch [1262/1500], Loss: 0.3952, MAE: 0.4551, RMSE: 0.6380\n",
            "Epoch [1263/1500], Loss: 0.3952, MAE: 0.4551, RMSE: 0.6379\n",
            "Epoch [1264/1500], Loss: 0.3951, MAE: 0.4551, RMSE: 0.6379\n",
            "Epoch [1265/1500], Loss: 0.3951, MAE: 0.4550, RMSE: 0.6378\n",
            "Epoch [1266/1500], Loss: 0.3950, MAE: 0.4550, RMSE: 0.6378\n",
            "Epoch [1267/1500], Loss: 0.3949, MAE: 0.4549, RMSE: 0.6377\n",
            "Epoch [1268/1500], Loss: 0.3949, MAE: 0.4549, RMSE: 0.6377\n",
            "Epoch [1269/1500], Loss: 0.3948, MAE: 0.4548, RMSE: 0.6376\n",
            "Epoch [1270/1500], Loss: 0.3948, MAE: 0.4548, RMSE: 0.6375\n",
            "Epoch [1271/1500], Loss: 0.3947, MAE: 0.4547, RMSE: 0.6375\n",
            "Epoch [1272/1500], Loss: 0.3947, MAE: 0.4547, RMSE: 0.6374\n",
            "Epoch [1273/1500], Loss: 0.3946, MAE: 0.4546, RMSE: 0.6374\n",
            "Epoch [1274/1500], Loss: 0.3946, MAE: 0.4546, RMSE: 0.6373\n",
            "Epoch [1275/1500], Loss: 0.3945, MAE: 0.4545, RMSE: 0.6373\n",
            "Epoch [1276/1500], Loss: 0.3944, MAE: 0.4545, RMSE: 0.6372\n",
            "Epoch [1277/1500], Loss: 0.3944, MAE: 0.4544, RMSE: 0.6372\n",
            "Epoch [1278/1500], Loss: 0.3943, MAE: 0.4544, RMSE: 0.6371\n",
            "Epoch [1279/1500], Loss: 0.3943, MAE: 0.4544, RMSE: 0.6370\n",
            "Epoch [1280/1500], Loss: 0.3942, MAE: 0.4543, RMSE: 0.6370\n",
            "Epoch [1281/1500], Loss: 0.3942, MAE: 0.4543, RMSE: 0.6369\n",
            "Epoch [1282/1500], Loss: 0.3941, MAE: 0.4542, RMSE: 0.6369\n",
            "Epoch [1283/1500], Loss: 0.3941, MAE: 0.4542, RMSE: 0.6368\n",
            "Epoch [1284/1500], Loss: 0.3940, MAE: 0.4541, RMSE: 0.6368\n",
            "Epoch [1285/1500], Loss: 0.3939, MAE: 0.4541, RMSE: 0.6367\n",
            "Epoch [1286/1500], Loss: 0.3939, MAE: 0.4540, RMSE: 0.6367\n",
            "Epoch [1287/1500], Loss: 0.3938, MAE: 0.4540, RMSE: 0.6366\n",
            "Epoch [1288/1500], Loss: 0.3938, MAE: 0.4539, RMSE: 0.6366\n",
            "Epoch [1289/1500], Loss: 0.3937, MAE: 0.4539, RMSE: 0.6365\n",
            "Epoch [1290/1500], Loss: 0.3937, MAE: 0.4539, RMSE: 0.6364\n",
            "Epoch [1291/1500], Loss: 0.3936, MAE: 0.4538, RMSE: 0.6364\n",
            "Epoch [1292/1500], Loss: 0.3936, MAE: 0.4538, RMSE: 0.6363\n",
            "Epoch [1293/1500], Loss: 0.3935, MAE: 0.4537, RMSE: 0.6363\n",
            "Epoch [1294/1500], Loss: 0.3935, MAE: 0.4537, RMSE: 0.6362\n",
            "Epoch [1295/1500], Loss: 0.3934, MAE: 0.4536, RMSE: 0.6362\n",
            "Epoch [1296/1500], Loss: 0.3934, MAE: 0.4536, RMSE: 0.6361\n",
            "Epoch [1297/1500], Loss: 0.3933, MAE: 0.4535, RMSE: 0.6361\n",
            "Epoch [1298/1500], Loss: 0.3932, MAE: 0.4535, RMSE: 0.6360\n",
            "Epoch [1299/1500], Loss: 0.3932, MAE: 0.4535, RMSE: 0.6360\n",
            "Epoch [1300/1500], Loss: 0.3931, MAE: 0.4534, RMSE: 0.6359\n",
            "Epoch [1301/1500], Loss: 0.3931, MAE: 0.4534, RMSE: 0.6359\n",
            "Epoch [1302/1500], Loss: 0.3930, MAE: 0.4533, RMSE: 0.6358\n",
            "Epoch [1303/1500], Loss: 0.3930, MAE: 0.4533, RMSE: 0.6358\n",
            "Epoch [1304/1500], Loss: 0.3929, MAE: 0.4532, RMSE: 0.6357\n",
            "Epoch [1305/1500], Loss: 0.3929, MAE: 0.4532, RMSE: 0.6357\n",
            "Epoch [1306/1500], Loss: 0.3928, MAE: 0.4532, RMSE: 0.6356\n",
            "Epoch [1307/1500], Loss: 0.3928, MAE: 0.4531, RMSE: 0.6356\n",
            "Epoch [1308/1500], Loss: 0.3927, MAE: 0.4531, RMSE: 0.6355\n",
            "Epoch [1309/1500], Loss: 0.3927, MAE: 0.4530, RMSE: 0.6355\n",
            "Epoch [1310/1500], Loss: 0.3926, MAE: 0.4530, RMSE: 0.6354\n",
            "Epoch [1311/1500], Loss: 0.3926, MAE: 0.4530, RMSE: 0.6354\n",
            "Epoch [1312/1500], Loss: 0.3925, MAE: 0.4529, RMSE: 0.6353\n",
            "Epoch [1313/1500], Loss: 0.3925, MAE: 0.4529, RMSE: 0.6353\n",
            "Epoch [1314/1500], Loss: 0.3924, MAE: 0.4528, RMSE: 0.6352\n",
            "Epoch [1315/1500], Loss: 0.3924, MAE: 0.4528, RMSE: 0.6352\n",
            "Epoch [1316/1500], Loss: 0.3923, MAE: 0.4528, RMSE: 0.6351\n",
            "Epoch [1317/1500], Loss: 0.3922, MAE: 0.4527, RMSE: 0.6351\n",
            "Epoch [1318/1500], Loss: 0.3922, MAE: 0.4527, RMSE: 0.6350\n",
            "Epoch [1319/1500], Loss: 0.3921, MAE: 0.4526, RMSE: 0.6350\n",
            "Epoch [1320/1500], Loss: 0.3921, MAE: 0.4526, RMSE: 0.6349\n",
            "Epoch [1321/1500], Loss: 0.3920, MAE: 0.4525, RMSE: 0.6349\n",
            "Epoch [1322/1500], Loss: 0.3920, MAE: 0.4525, RMSE: 0.6348\n",
            "Epoch [1323/1500], Loss: 0.3919, MAE: 0.4525, RMSE: 0.6348\n",
            "Epoch [1324/1500], Loss: 0.3919, MAE: 0.4524, RMSE: 0.6347\n",
            "Epoch [1325/1500], Loss: 0.3918, MAE: 0.4524, RMSE: 0.6347\n",
            "Epoch [1326/1500], Loss: 0.3918, MAE: 0.4523, RMSE: 0.6346\n",
            "Epoch [1327/1500], Loss: 0.3917, MAE: 0.4523, RMSE: 0.6346\n",
            "Epoch [1328/1500], Loss: 0.3917, MAE: 0.4522, RMSE: 0.6345\n",
            "Epoch [1329/1500], Loss: 0.3916, MAE: 0.4522, RMSE: 0.6345\n",
            "Epoch [1330/1500], Loss: 0.3916, MAE: 0.4522, RMSE: 0.6344\n",
            "Epoch [1331/1500], Loss: 0.3915, MAE: 0.4521, RMSE: 0.6344\n",
            "Epoch [1332/1500], Loss: 0.3915, MAE: 0.4521, RMSE: 0.6343\n",
            "Epoch [1333/1500], Loss: 0.3914, MAE: 0.4520, RMSE: 0.6343\n",
            "Epoch [1334/1500], Loss: 0.3914, MAE: 0.4520, RMSE: 0.6342\n",
            "Epoch [1335/1500], Loss: 0.3913, MAE: 0.4520, RMSE: 0.6342\n",
            "Epoch [1336/1500], Loss: 0.3913, MAE: 0.4519, RMSE: 0.6341\n",
            "Epoch [1337/1500], Loss: 0.3912, MAE: 0.4519, RMSE: 0.6341\n",
            "Epoch [1338/1500], Loss: 0.3912, MAE: 0.4518, RMSE: 0.6340\n",
            "Epoch [1339/1500], Loss: 0.3911, MAE: 0.4518, RMSE: 0.6340\n",
            "Epoch [1340/1500], Loss: 0.3911, MAE: 0.4518, RMSE: 0.6339\n",
            "Epoch [1341/1500], Loss: 0.3910, MAE: 0.4517, RMSE: 0.6339\n",
            "Epoch [1342/1500], Loss: 0.3910, MAE: 0.4517, RMSE: 0.6338\n",
            "Epoch [1343/1500], Loss: 0.3909, MAE: 0.4516, RMSE: 0.6338\n",
            "Epoch [1344/1500], Loss: 0.3909, MAE: 0.4516, RMSE: 0.6337\n",
            "Epoch [1345/1500], Loss: 0.3908, MAE: 0.4515, RMSE: 0.6337\n",
            "Epoch [1346/1500], Loss: 0.3908, MAE: 0.4515, RMSE: 0.6336\n",
            "Epoch [1347/1500], Loss: 0.3907, MAE: 0.4515, RMSE: 0.6336\n",
            "Epoch [1348/1500], Loss: 0.3907, MAE: 0.4514, RMSE: 0.6335\n",
            "Epoch [1349/1500], Loss: 0.3906, MAE: 0.4514, RMSE: 0.6335\n",
            "Epoch [1350/1500], Loss: 0.3906, MAE: 0.4513, RMSE: 0.6335\n",
            "Epoch [1351/1500], Loss: 0.3905, MAE: 0.4513, RMSE: 0.6334\n",
            "Epoch [1352/1500], Loss: 0.3905, MAE: 0.4513, RMSE: 0.6334\n",
            "Epoch [1353/1500], Loss: 0.3904, MAE: 0.4512, RMSE: 0.6333\n",
            "Epoch [1354/1500], Loss: 0.3904, MAE: 0.4512, RMSE: 0.6333\n",
            "Epoch [1355/1500], Loss: 0.3903, MAE: 0.4512, RMSE: 0.6332\n",
            "Epoch [1356/1500], Loss: 0.3903, MAE: 0.4511, RMSE: 0.6332\n",
            "Epoch [1357/1500], Loss: 0.3902, MAE: 0.4511, RMSE: 0.6331\n",
            "Epoch [1358/1500], Loss: 0.3902, MAE: 0.4510, RMSE: 0.6331\n",
            "Epoch [1359/1500], Loss: 0.3901, MAE: 0.4510, RMSE: 0.6330\n",
            "Epoch [1360/1500], Loss: 0.3901, MAE: 0.4510, RMSE: 0.6330\n",
            "Epoch [1361/1500], Loss: 0.3900, MAE: 0.4509, RMSE: 0.6329\n",
            "Epoch [1362/1500], Loss: 0.3900, MAE: 0.4509, RMSE: 0.6329\n",
            "Epoch [1363/1500], Loss: 0.3899, MAE: 0.4509, RMSE: 0.6328\n",
            "Epoch [1364/1500], Loss: 0.3899, MAE: 0.4508, RMSE: 0.6328\n",
            "Epoch [1365/1500], Loss: 0.3899, MAE: 0.4508, RMSE: 0.6328\n",
            "Epoch [1366/1500], Loss: 0.3898, MAE: 0.4508, RMSE: 0.6327\n",
            "Epoch [1367/1500], Loss: 0.3898, MAE: 0.4507, RMSE: 0.6327\n",
            "Epoch [1368/1500], Loss: 0.3897, MAE: 0.4507, RMSE: 0.6326\n",
            "Epoch [1369/1500], Loss: 0.3897, MAE: 0.4506, RMSE: 0.6326\n",
            "Epoch [1370/1500], Loss: 0.3896, MAE: 0.4506, RMSE: 0.6325\n",
            "Epoch [1371/1500], Loss: 0.3896, MAE: 0.4506, RMSE: 0.6325\n",
            "Epoch [1372/1500], Loss: 0.3895, MAE: 0.4505, RMSE: 0.6324\n",
            "Epoch [1373/1500], Loss: 0.3895, MAE: 0.4505, RMSE: 0.6324\n",
            "Epoch [1374/1500], Loss: 0.3894, MAE: 0.4505, RMSE: 0.6324\n",
            "Epoch [1375/1500], Loss: 0.3894, MAE: 0.4504, RMSE: 0.6323\n",
            "Epoch [1376/1500], Loss: 0.3893, MAE: 0.4504, RMSE: 0.6323\n",
            "Epoch [1377/1500], Loss: 0.3893, MAE: 0.4504, RMSE: 0.6322\n",
            "Epoch [1378/1500], Loss: 0.3893, MAE: 0.4503, RMSE: 0.6322\n",
            "Epoch [1379/1500], Loss: 0.3892, MAE: 0.4503, RMSE: 0.6321\n",
            "Epoch [1380/1500], Loss: 0.3892, MAE: 0.4502, RMSE: 0.6321\n",
            "Epoch [1381/1500], Loss: 0.3891, MAE: 0.4502, RMSE: 0.6320\n",
            "Epoch [1382/1500], Loss: 0.3891, MAE: 0.4502, RMSE: 0.6320\n",
            "Epoch [1383/1500], Loss: 0.3890, MAE: 0.4501, RMSE: 0.6320\n",
            "Epoch [1384/1500], Loss: 0.3890, MAE: 0.4501, RMSE: 0.6319\n",
            "Epoch [1385/1500], Loss: 0.3889, MAE: 0.4501, RMSE: 0.6319\n",
            "Epoch [1386/1500], Loss: 0.3889, MAE: 0.4500, RMSE: 0.6318\n",
            "Epoch [1387/1500], Loss: 0.3888, MAE: 0.4500, RMSE: 0.6318\n",
            "Epoch [1388/1500], Loss: 0.3888, MAE: 0.4499, RMSE: 0.6317\n",
            "Epoch [1389/1500], Loss: 0.3888, MAE: 0.4499, RMSE: 0.6317\n",
            "Epoch [1390/1500], Loss: 0.3887, MAE: 0.4499, RMSE: 0.6316\n",
            "Epoch [1391/1500], Loss: 0.3887, MAE: 0.4498, RMSE: 0.6316\n",
            "Epoch [1392/1500], Loss: 0.3886, MAE: 0.4498, RMSE: 0.6316\n",
            "Epoch [1393/1500], Loss: 0.3886, MAE: 0.4498, RMSE: 0.6315\n",
            "Epoch [1394/1500], Loss: 0.3885, MAE: 0.4497, RMSE: 0.6315\n",
            "Epoch [1395/1500], Loss: 0.3885, MAE: 0.4497, RMSE: 0.6314\n",
            "Epoch [1396/1500], Loss: 0.3884, MAE: 0.4497, RMSE: 0.6314\n",
            "Epoch [1397/1500], Loss: 0.3884, MAE: 0.4496, RMSE: 0.6313\n",
            "Epoch [1398/1500], Loss: 0.3883, MAE: 0.4496, RMSE: 0.6313\n",
            "Epoch [1399/1500], Loss: 0.3883, MAE: 0.4495, RMSE: 0.6313\n",
            "Epoch [1400/1500], Loss: 0.3883, MAE: 0.4495, RMSE: 0.6312\n",
            "Epoch [1401/1500], Loss: 0.3882, MAE: 0.4495, RMSE: 0.6312\n",
            "Epoch [1402/1500], Loss: 0.3882, MAE: 0.4494, RMSE: 0.6311\n",
            "Epoch [1403/1500], Loss: 0.3881, MAE: 0.4494, RMSE: 0.6311\n",
            "Epoch [1404/1500], Loss: 0.3881, MAE: 0.4494, RMSE: 0.6310\n",
            "Epoch [1405/1500], Loss: 0.3880, MAE: 0.4493, RMSE: 0.6310\n",
            "Epoch [1406/1500], Loss: 0.3880, MAE: 0.4493, RMSE: 0.6310\n",
            "Epoch [1407/1500], Loss: 0.3879, MAE: 0.4493, RMSE: 0.6309\n",
            "Epoch [1408/1500], Loss: 0.3879, MAE: 0.4492, RMSE: 0.6309\n",
            "Epoch [1409/1500], Loss: 0.3878, MAE: 0.4492, RMSE: 0.6308\n",
            "Epoch [1410/1500], Loss: 0.3878, MAE: 0.4491, RMSE: 0.6308\n",
            "Epoch [1411/1500], Loss: 0.3878, MAE: 0.4491, RMSE: 0.6307\n",
            "Epoch [1412/1500], Loss: 0.3877, MAE: 0.4491, RMSE: 0.6307\n",
            "Epoch [1413/1500], Loss: 0.3877, MAE: 0.4490, RMSE: 0.6307\n",
            "Epoch [1414/1500], Loss: 0.3876, MAE: 0.4490, RMSE: 0.6306\n",
            "Epoch [1415/1500], Loss: 0.3876, MAE: 0.4490, RMSE: 0.6306\n",
            "Epoch [1416/1500], Loss: 0.3875, MAE: 0.4489, RMSE: 0.6305\n",
            "Epoch [1417/1500], Loss: 0.3875, MAE: 0.4489, RMSE: 0.6305\n",
            "Epoch [1418/1500], Loss: 0.3874, MAE: 0.4488, RMSE: 0.6304\n",
            "Epoch [1419/1500], Loss: 0.3874, MAE: 0.4488, RMSE: 0.6304\n",
            "Epoch [1420/1500], Loss: 0.3874, MAE: 0.4488, RMSE: 0.6303\n",
            "Epoch [1421/1500], Loss: 0.3873, MAE: 0.4487, RMSE: 0.6303\n",
            "Epoch [1422/1500], Loss: 0.3873, MAE: 0.4487, RMSE: 0.6303\n",
            "Epoch [1423/1500], Loss: 0.3872, MAE: 0.4487, RMSE: 0.6302\n",
            "Epoch [1424/1500], Loss: 0.3872, MAE: 0.4486, RMSE: 0.6302\n",
            "Epoch [1425/1500], Loss: 0.3871, MAE: 0.4486, RMSE: 0.6301\n",
            "Epoch [1426/1500], Loss: 0.3871, MAE: 0.4486, RMSE: 0.6301\n",
            "Epoch [1427/1500], Loss: 0.3871, MAE: 0.4485, RMSE: 0.6300\n",
            "Epoch [1428/1500], Loss: 0.3870, MAE: 0.4485, RMSE: 0.6300\n",
            "Epoch [1429/1500], Loss: 0.3870, MAE: 0.4485, RMSE: 0.6300\n",
            "Epoch [1430/1500], Loss: 0.3869, MAE: 0.4484, RMSE: 0.6299\n",
            "Epoch [1431/1500], Loss: 0.3869, MAE: 0.4484, RMSE: 0.6299\n",
            "Epoch [1432/1500], Loss: 0.3868, MAE: 0.4484, RMSE: 0.6298\n",
            "Epoch [1433/1500], Loss: 0.3868, MAE: 0.4483, RMSE: 0.6298\n",
            "Epoch [1434/1500], Loss: 0.3868, MAE: 0.4483, RMSE: 0.6298\n",
            "Epoch [1435/1500], Loss: 0.3867, MAE: 0.4483, RMSE: 0.6297\n",
            "Epoch [1436/1500], Loss: 0.3867, MAE: 0.4482, RMSE: 0.6297\n",
            "Epoch [1437/1500], Loss: 0.3866, MAE: 0.4482, RMSE: 0.6296\n",
            "Epoch [1438/1500], Loss: 0.3866, MAE: 0.4482, RMSE: 0.6296\n",
            "Epoch [1439/1500], Loss: 0.3866, MAE: 0.4481, RMSE: 0.6295\n",
            "Epoch [1440/1500], Loss: 0.3865, MAE: 0.4481, RMSE: 0.6295\n",
            "Epoch [1441/1500], Loss: 0.3865, MAE: 0.4481, RMSE: 0.6295\n",
            "Epoch [1442/1500], Loss: 0.3864, MAE: 0.4480, RMSE: 0.6294\n",
            "Epoch [1443/1500], Loss: 0.3864, MAE: 0.4480, RMSE: 0.6294\n",
            "Epoch [1444/1500], Loss: 0.3863, MAE: 0.4480, RMSE: 0.6293\n",
            "Epoch [1445/1500], Loss: 0.3863, MAE: 0.4479, RMSE: 0.6293\n",
            "Epoch [1446/1500], Loss: 0.3863, MAE: 0.4479, RMSE: 0.6293\n",
            "Epoch [1447/1500], Loss: 0.3862, MAE: 0.4479, RMSE: 0.6292\n",
            "Epoch [1448/1500], Loss: 0.3862, MAE: 0.4478, RMSE: 0.6292\n",
            "Epoch [1449/1500], Loss: 0.3861, MAE: 0.4478, RMSE: 0.6291\n",
            "Epoch [1450/1500], Loss: 0.3861, MAE: 0.4478, RMSE: 0.6291\n",
            "Epoch [1451/1500], Loss: 0.3861, MAE: 0.4477, RMSE: 0.6291\n",
            "Epoch [1452/1500], Loss: 0.3860, MAE: 0.4477, RMSE: 0.6290\n",
            "Epoch [1453/1500], Loss: 0.3860, MAE: 0.4477, RMSE: 0.6290\n",
            "Epoch [1454/1500], Loss: 0.3859, MAE: 0.4476, RMSE: 0.6289\n",
            "Epoch [1455/1500], Loss: 0.3859, MAE: 0.4476, RMSE: 0.6289\n",
            "Epoch [1456/1500], Loss: 0.3859, MAE: 0.4476, RMSE: 0.6289\n",
            "Epoch [1457/1500], Loss: 0.3858, MAE: 0.4475, RMSE: 0.6288\n",
            "Epoch [1458/1500], Loss: 0.3858, MAE: 0.4475, RMSE: 0.6288\n",
            "Epoch [1459/1500], Loss: 0.3857, MAE: 0.4475, RMSE: 0.6287\n",
            "Epoch [1460/1500], Loss: 0.3857, MAE: 0.4474, RMSE: 0.6287\n",
            "Epoch [1461/1500], Loss: 0.3857, MAE: 0.4474, RMSE: 0.6287\n",
            "Epoch [1462/1500], Loss: 0.3856, MAE: 0.4474, RMSE: 0.6286\n",
            "Epoch [1463/1500], Loss: 0.3856, MAE: 0.4474, RMSE: 0.6286\n",
            "Epoch [1464/1500], Loss: 0.3856, MAE: 0.4473, RMSE: 0.6285\n",
            "Epoch [1465/1500], Loss: 0.3855, MAE: 0.4473, RMSE: 0.6285\n",
            "Epoch [1466/1500], Loss: 0.3855, MAE: 0.4473, RMSE: 0.6285\n",
            "Epoch [1467/1500], Loss: 0.3854, MAE: 0.4472, RMSE: 0.6284\n",
            "Epoch [1468/1500], Loss: 0.3854, MAE: 0.4472, RMSE: 0.6284\n",
            "Epoch [1469/1500], Loss: 0.3854, MAE: 0.4472, RMSE: 0.6283\n",
            "Epoch [1470/1500], Loss: 0.3853, MAE: 0.4471, RMSE: 0.6283\n",
            "Epoch [1471/1500], Loss: 0.3853, MAE: 0.4471, RMSE: 0.6283\n",
            "Epoch [1472/1500], Loss: 0.3853, MAE: 0.4471, RMSE: 0.6282\n",
            "Epoch [1473/1500], Loss: 0.3852, MAE: 0.4471, RMSE: 0.6282\n",
            "Epoch [1474/1500], Loss: 0.3852, MAE: 0.4470, RMSE: 0.6281\n",
            "Epoch [1475/1500], Loss: 0.3851, MAE: 0.4470, RMSE: 0.6281\n",
            "Epoch [1476/1500], Loss: 0.3851, MAE: 0.4470, RMSE: 0.6281\n",
            "Epoch [1477/1500], Loss: 0.3851, MAE: 0.4469, RMSE: 0.6280\n",
            "Epoch [1478/1500], Loss: 0.3850, MAE: 0.4469, RMSE: 0.6280\n",
            "Epoch [1479/1500], Loss: 0.3850, MAE: 0.4469, RMSE: 0.6279\n",
            "Epoch [1480/1500], Loss: 0.3850, MAE: 0.4468, RMSE: 0.6279\n",
            "Epoch [1481/1500], Loss: 0.3849, MAE: 0.4468, RMSE: 0.6279\n",
            "Epoch [1482/1500], Loss: 0.3849, MAE: 0.4468, RMSE: 0.6278\n",
            "Epoch [1483/1500], Loss: 0.3849, MAE: 0.4468, RMSE: 0.6278\n",
            "Epoch [1484/1500], Loss: 0.3848, MAE: 0.4467, RMSE: 0.6277\n",
            "Epoch [1485/1500], Loss: 0.3848, MAE: 0.4467, RMSE: 0.6277\n",
            "Epoch [1486/1500], Loss: 0.3847, MAE: 0.4467, RMSE: 0.6277\n",
            "Epoch [1487/1500], Loss: 0.3847, MAE: 0.4466, RMSE: 0.6276\n",
            "Epoch [1488/1500], Loss: 0.3847, MAE: 0.4466, RMSE: 0.6276\n",
            "Epoch [1489/1500], Loss: 0.3846, MAE: 0.4466, RMSE: 0.6275\n",
            "Epoch [1490/1500], Loss: 0.3846, MAE: 0.4465, RMSE: 0.6275\n",
            "Epoch [1491/1500], Loss: 0.3846, MAE: 0.4465, RMSE: 0.6275\n",
            "Epoch [1492/1500], Loss: 0.3845, MAE: 0.4465, RMSE: 0.6274\n",
            "Epoch [1493/1500], Loss: 0.3845, MAE: 0.4464, RMSE: 0.6274\n",
            "Epoch [1494/1500], Loss: 0.3845, MAE: 0.4464, RMSE: 0.6273\n",
            "Epoch [1495/1500], Loss: 0.3844, MAE: 0.4464, RMSE: 0.6273\n",
            "Epoch [1496/1500], Loss: 0.3844, MAE: 0.4463, RMSE: 0.6273\n",
            "Epoch [1497/1500], Loss: 0.3843, MAE: 0.4463, RMSE: 0.6272\n",
            "Epoch [1498/1500], Loss: 0.3843, MAE: 0.4463, RMSE: 0.6272\n",
            "Epoch [1499/1500], Loss: 0.3843, MAE: 0.4462, RMSE: 0.6272\n",
            "Epoch [1500/1500], Loss: 0.3842, MAE: 0.4462, RMSE: 0.6271\n",
            "Test Loss: 0.3933, Test MAE: 0.4462, Test RMSE: 0.6271\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}